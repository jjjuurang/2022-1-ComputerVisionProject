{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet101_Training_CIFAR10_normalize_batch32_epoch100.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ResNet Training (CIFAR - 10)\n",
        "1. Import Library & Define Resnet model"
      ],
      "metadata": {
        "id": "uTGHKmVakOlS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zO62BlwchvVY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class Conv2dAuto(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
        "        \n",
        "\n",
        "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels, self.out_channels =  in_channels, out_channels\n",
        "        self.blocks = nn.Identity()\n",
        "        self.shortcut = nn.Identity()   \n",
        "    \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
        "        x = self.blocks(x)\n",
        "        x += residual\n",
        "        return x\n",
        "    \n",
        "    @property\n",
        "    def should_apply_shortcut(self):\n",
        "        return self.in_channels != self.out_channels\n",
        "    \n",
        "\n",
        "\n",
        "class ResNetResidualBlock(ResidualBlock):\n",
        "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels)\n",
        "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
        "        self.shortcut = nn.Sequential(OrderedDict(\n",
        "        {\n",
        "            'conv' : nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1,\n",
        "                      stride=self.downsampling, bias=False),\n",
        "            'bn' : nn.BatchNorm2d(self.expanded_channels)\n",
        "            \n",
        "        })) if self.should_apply_shortcut else None\n",
        "        \n",
        "        \n",
        "    @property\n",
        "    def expanded_channels(self):\n",
        "        return self.out_channels * self.expansion\n",
        "    \n",
        "    @property\n",
        "    def should_apply_shortcut(self):\n",
        "        return self.in_channels != self.expanded_channels\n",
        "\n",
        "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
        "    return nn.Sequential(OrderedDict({'conv': conv(in_channels, out_channels, *args, **kwargs), \n",
        "                          'bn': nn.BatchNorm2d(out_channels) }))\n",
        "\n",
        "class ResNetBasicBlock(ResNetResidualBlock):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, activation=nn.ReLU, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
        "        self.blocks = nn.Sequential(\n",
        "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
        "            activation(),\n",
        "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
        "        )\n",
        "        \n",
        "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, activation=nn.ReLU, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
        "        self.blocks = nn.Sequential(\n",
        "           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
        "             activation(),\n",
        "             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
        "             activation(),\n",
        "             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
        "        )\n",
        "        \n",
        "class ResNetLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
        "        downsampling = 2 if in_channels != out_channels else 1\n",
        "        \n",
        "        self.blocks = nn.Sequential(\n",
        "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
        "            *[block(out_channels * block.expansion, \n",
        "                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        return x\n",
        "    \n",
        "class ResNetEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet encoder composed by increasing different layers with increasing features.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, blocks_sizes=[64, 128, 256, 512], deepths=[2,2,2,2], \n",
        "                 activation=nn.ReLU, block=ResNetBasicBlock, *args,**kwargs):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.blocks_sizes = blocks_sizes\n",
        "        \n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.blocks_sizes[0], kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(self.blocks_sizes[0]),\n",
        "            activation(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
        "        self.blocks = nn.ModuleList([ \n",
        "            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=deepths[0], activation=activation, \n",
        "                        block=block,  *args, **kwargs),\n",
        "            *[ResNetLayer(in_channels * block.expansion, \n",
        "                          out_channels, n=n, activation=activation, \n",
        "                          block=block, *args, **kwargs) \n",
        "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]       \n",
        "        ])\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.gate(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class ResnetDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents the tail of ResNet. It performs a global pooling and maps the output to the\n",
        "    correct class by using a fully connected layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, n_classes):\n",
        "        super().__init__()\n",
        "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.decoder = nn.Linear(in_features, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "    \n",
        "class ResNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, n_classes, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.encoder = ResNetEncoder(in_channels, *args, **kwargs)\n",
        "        self.decoder = ResnetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "    \n",
        "def resnet18(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBasicBlock, deepths=[2, 2, 2, 2])\n",
        "\n",
        "def resnet34(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBasicBlock, deepths=[3, 4, 6, 3])\n",
        "\n",
        "def resnet50(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBottleNeckBlock, deepths=[3, 4, 6, 3])\n",
        "\n",
        "def resnet101(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBottleNeckBlock, deepths=[3, 4, 23, 3])\n",
        "\n",
        "def resnet152(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBottleNeckBlock, deepths=[3, 8, 36, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Download CIFAR-10"
      ],
      "metadata": {
        "id": "peNdZNJrkVjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download CIFAR10 DATASET\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "\n",
        "    transforms.ToTensor(), normalize\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(), normalize\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gie-we8Th8iX",
        "outputId": "23a71346-d9df-462c-c801-d9d97586090d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. CUDA setting & Define Train,Test function"
      ],
      "metadata": {
        "id": "oLG1W3k6kZwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gpu setting & define Training function\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "net = resnet101(3,10)           #resnet101 선언\n",
        "net = net.to(device)\n",
        "net = torch.nn.DataParallel(net)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "learning_rate = 0.1\n",
        "file_name = 'resnet101_cifar10.pt'\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\n[ Train epoch: %d ]' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        benign_outputs = net(inputs)\n",
        "        loss = criterion(benign_outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = benign_outputs.max(1)\n",
        "\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('\\nCurrent batch:', str(batch_idx))\n",
        "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
        "            print('Current benign train loss:', loss.item())\n",
        "\n",
        "    print('\\nTotal benign train accuarcy:', 100. * correct / total)\n",
        "    print('Total benign train loss:', train_loss)\n",
        "    total_accuracy = 100. * correct / total;\n",
        "\n",
        "    return total_accuracy , train_loss\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    print('\\n[ Test epoch: %d ]' % epoch)\n",
        "    net.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print('\\nTest accuarcy:', 100. * correct / total)\n",
        "    print('Test average loss:', loss / total)\n",
        "    total_accuracy = 100. * correct / total;\n",
        "    test_loss = loss / total\n",
        "\n",
        "    state = {\n",
        "        'net': net.state_dict()\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/' + file_name)\n",
        "    print('Model Saved!')\n",
        "\n",
        "    return total_accuracy , test_loss\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lr = learning_rate\n",
        "    if epoch >= 100:\n",
        "        lr /= 10\n",
        "    if epoch >= 150:\n",
        "        lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "wxT1pfXwiAGD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Run Training & Save log"
      ],
      "metadata": {
        "id": "_duH55imkhMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for epoch in range(0, 60):\n",
        "log_train_total_accuracy =[]\n",
        "log_train_total_loss = []\n",
        "log_test_total_accuracy =[]\n",
        "log_test_total_loss =[]\n",
        "\n",
        "\n",
        "for epoch in range(0, 100):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    train_total_accuracy, train_total_loss = train(epoch)\n",
        "    test_total_accuracy, test_total_loss = test(epoch)\n",
        "    log_train_total_accuracy.append(train_total_accuracy)\n",
        "    log_train_total_loss.append(train_total_loss)\n",
        "    log_test_total_accuracy.append(test_total_accuracy)\n",
        "    log_test_total_loss.append(test_total_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7vBS8DEiEBD",
        "outputId": "423e195f-f58f-470c-c2bf-d12a425dc64b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Current benign train loss: 1.1637345552444458\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9354060292243958\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.6835941076278687\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1555253267288208\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8008561730384827\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3622184991836548\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.1826459169387817\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9004433751106262\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5116458535194397\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.6208518743515015\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9932085871696472\n",
            "\n",
            "Total benign train accuarcy: 64.63\n",
            "Total benign train loss: 1628.9436132013798\n",
            "\n",
            "[ Test epoch: 33 ]\n",
            "\n",
            "Test accuarcy: 60.24\n",
            "Test average loss: 0.03888827961087227\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 34 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7585808634757996\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1014479398727417\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1282676458358765\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.902186930179596\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7221278548240662\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8325907588005066\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9399507641792297\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.025682806968689\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7724909782409668\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7331002950668335\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.446707010269165\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1926095485687256\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1878154277801514\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0366685390472412\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3972450494766235\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9682267308235168\n",
            "\n",
            "Total benign train accuarcy: 64.954\n",
            "Total benign train loss: 1634.937134951353\n",
            "\n",
            "[ Test epoch: 34 ]\n",
            "\n",
            "Test accuarcy: 64.72\n",
            "Test average loss: 0.03237849652469158\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 35 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2482991218566895\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2542474269866943\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9291430711746216\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0021735429763794\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0303490161895752\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0580382347106934\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9444804191589355\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7131720781326294\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8392765522003174\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.13485586643219\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6164687871932983\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1853281259536743\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8175783753395081\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.91932612657547\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8664814829826355\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0597957372665405\n",
            "\n",
            "Total benign train accuarcy: 65.29\n",
            "Total benign train loss: 1615.5917722284794\n",
            "\n",
            "[ Test epoch: 35 ]\n",
            "\n",
            "Test accuarcy: 68.65\n",
            "Test average loss: 0.029876072803139687\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 36 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8969690203666687\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5448510646820068\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.239507794380188\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0795180797576904\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1878118515014648\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9097592234611511\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8302255272865295\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0609924793243408\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3521121740341187\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.183974266052246\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8152106404304504\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.962834894657135\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.8575429320335388\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7588221430778503\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.866995096206665\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2073924541473389\n",
            "\n",
            "Total benign train accuarcy: 65.198\n",
            "Total benign train loss: 1624.3486448526382\n",
            "\n",
            "[ Test epoch: 36 ]\n",
            "\n",
            "Test accuarcy: 67.76\n",
            "Test average loss: 0.02906727485060692\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 37 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6788848638534546\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1747649908065796\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3181079626083374\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.7748548984527588\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0482027530670166\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6275081634521484\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7302379608154297\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7024461627006531\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9743008017539978\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9482178092002869\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.064566731452942\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0518264770507812\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3486778736114502\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7034280300140381\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.050774335861206\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1661113500595093\n",
            "\n",
            "Total benign train accuarcy: 65.366\n",
            "Total benign train loss: 1617.4029343128204\n",
            "\n",
            "[ Test epoch: 37 ]\n",
            "\n",
            "Test accuarcy: 66.14\n",
            "Test average loss: 0.03119462400972843\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 38 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.850921630859375\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.62998628616333\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7994768619537354\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.190246820449829\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.6286944150924683\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.508951187133789\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.00331711769104\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7425341010093689\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9958940744400024\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.106230616569519\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9086692929267883\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0347338914871216\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.127776861190796\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.837825357913971\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9730603694915771\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7461342215538025\n",
            "\n",
            "Total benign train accuarcy: 65.418\n",
            "Total benign train loss: 1606.7527054548264\n",
            "\n",
            "[ Test epoch: 38 ]\n",
            "\n",
            "Test accuarcy: 63.57\n",
            "Test average loss: 0.03474110432267189\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 39 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8052560091018677\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.801623523235321\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8566737771034241\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1630032062530518\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0639785528182983\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8453982472419739\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.6248464584350586\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8368618488311768\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9658121466636658\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2377102375030518\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9623786211013794\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.302196741104126\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.889630138874054\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1832892894744873\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.5002533197402954\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2959578037261963\n",
            "\n",
            "Total benign train accuarcy: 65.296\n",
            "Total benign train loss: 1614.1037211418152\n",
            "\n",
            "[ Test epoch: 39 ]\n",
            "\n",
            "Test accuarcy: 63.35\n",
            "Test average loss: 0.03576869702339172\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 40 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9408344626426697\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8182594180107117\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9591506719589233\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.810283899307251\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3753254413604736\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9594963788986206\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9180229306221008\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8394688367843628\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1552679538726807\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9902681708335876\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0121395587921143\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0108164548873901\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3225586414337158\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8698710799217224\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.777055561542511\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9256848096847534\n",
            "\n",
            "Total benign train accuarcy: 65.306\n",
            "Total benign train loss: 1619.2214652001858\n",
            "\n",
            "[ Test epoch: 40 ]\n",
            "\n",
            "Test accuarcy: 60.45\n",
            "Test average loss: 0.03846890930831432\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 41 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0194666385650635\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9205110669136047\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9770422577857971\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8415986895561218\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.130026936531067\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2532718181610107\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9995383024215698\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8218240737915039\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8346813917160034\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2157001495361328\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0173579454421997\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3354897499084473\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7334209084510803\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2443228960037231\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7530975341796875\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0546529293060303\n",
            "\n",
            "Total benign train accuarcy: 65.532\n",
            "Total benign train loss: 1599.2726356685162\n",
            "\n",
            "[ Test epoch: 41 ]\n",
            "\n",
            "Test accuarcy: 65.66\n",
            "Test average loss: 0.03134393725991249\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 42 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.222293734550476\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.197993516921997\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9508440494537354\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6392750144004822\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0198968648910522\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9194000959396362\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1352523565292358\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.973755955696106\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9336373805999756\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8195359706878662\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.3368260860443115\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1045591831207275\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.5123969316482544\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2671030759811401\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9089133143424988\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3850154876708984\n",
            "\n",
            "Total benign train accuarcy: 65.616\n",
            "Total benign train loss: 1610.3486645519733\n",
            "\n",
            "[ Test epoch: 42 ]\n",
            "\n",
            "Test accuarcy: 67.59\n",
            "Test average loss: 0.029896009716391562\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 43 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0187982320785522\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.215778112411499\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8270751237869263\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1620663404464722\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.288864254951477\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.0450416803359985\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6568495035171509\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6649703979492188\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5978959798812866\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9392811059951782\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0388253927230835\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2945060729980469\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.8379757404327393\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.757605791091919\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1363270282745361\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8380120396614075\n",
            "\n",
            "Total benign train accuarcy: 65.54\n",
            "Total benign train loss: 1606.680177628994\n",
            "\n",
            "[ Test epoch: 43 ]\n",
            "\n",
            "Test accuarcy: 67.9\n",
            "Test average loss: 0.030270370319485665\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 44 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3068432807922363\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1124646663665771\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1725040674209595\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7757982611656189\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0321325063705444\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.1607916355133057\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0855196714401245\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7576994299888611\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.17423415184021\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1542866230010986\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8347412943840027\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1158506870269775\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0131969451904297\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8252573609352112\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9137517213821411\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9464384913444519\n",
            "\n",
            "Total benign train accuarcy: 65.698\n",
            "Total benign train loss: 1600.43694755435\n",
            "\n",
            "[ Test epoch: 44 ]\n",
            "\n",
            "Test accuarcy: 67.15\n",
            "Test average loss: 0.03118058749139309\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 45 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9296817779541016\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.892505407333374\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7695723176002502\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.846030592918396\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.175793170928955\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3461071252822876\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3581573963165283\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9844202399253845\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.1125242710113525\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.097840666770935\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9166702032089233\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8800443410873413\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6603316068649292\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8916480541229248\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.47131043672561646\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1700772047042847\n",
            "\n",
            "Total benign train accuarcy: 65.982\n",
            "Total benign train loss: 1594.2839897572994\n",
            "\n",
            "[ Test epoch: 45 ]\n",
            "\n",
            "Test accuarcy: 65.11\n",
            "Test average loss: 0.03356861881613731\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 46 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9613390564918518\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7344803214073181\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7031575441360474\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3460510969161987\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8530340790748596\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9198853373527527\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1073682308197021\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9743090867996216\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0277749300003052\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9969730973243713\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8780772089958191\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.16853928565979\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3795472383499146\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.2811894416809082\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0154937505722046\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8177753686904907\n",
            "\n",
            "Total benign train accuarcy: 65.938\n",
            "Total benign train loss: 1587.8543565273285\n",
            "\n",
            "[ Test epoch: 46 ]\n",
            "\n",
            "Test accuarcy: 69.36\n",
            "Test average loss: 0.028944093862175942\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 47 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.066296100616455\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8266018033027649\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0540928840637207\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.8966129422187805\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8851872682571411\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8986185789108276\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.446095585823059\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.562293291091919\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 0.9736790060997009\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.167305827140808\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7262493371963501\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0378406047821045\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1364171504974365\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8396539092063904\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.927051842212677\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8135905265808105\n",
            "\n",
            "Total benign train accuarcy: 65.73\n",
            "Total benign train loss: 1598.0760312080383\n",
            "\n",
            "[ Test epoch: 47 ]\n",
            "\n",
            "Test accuarcy: 67.95\n",
            "Test average loss: 0.030056731697916984\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 48 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7792032957077026\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0011483430862427\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.196283221244812\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8342857956886292\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.86811763048172\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0254013538360596\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2278037071228027\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.020539402961731\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1140360832214355\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9686722159385681\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9067292213439941\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9148202538490295\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2414814233779907\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.414303183555603\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9805704355239868\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3243345022201538\n",
            "\n",
            "Total benign train accuarcy: 65.99\n",
            "Total benign train loss: 1588.1661405563354\n",
            "\n",
            "[ Test epoch: 48 ]\n",
            "\n",
            "Test accuarcy: 69.73\n",
            "Test average loss: 0.028625020840764046\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 49 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9903901815414429\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8073645830154419\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6742062568664551\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0119973421096802\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.40625\n",
            "Current benign train loss: 1.5810714960098267\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9514328241348267\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.565273642539978\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8123564720153809\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.697605311870575\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9966973662376404\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9701941013336182\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0097821950912476\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.157001256942749\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2455672025680542\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9454026222229004\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8770105838775635\n",
            "\n",
            "Total benign train accuarcy: 66.1\n",
            "Total benign train loss: 1574.685079574585\n",
            "\n",
            "[ Test epoch: 49 ]\n",
            "\n",
            "Test accuarcy: 64.06\n",
            "Test average loss: 0.033774659115076065\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 50 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.1213270425796509\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.1071735620498657\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9521770477294922\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7257746458053589\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.5070635080337524\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2327337265014648\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.004146695137024\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5599638819694519\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1873246431350708\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1637433767318726\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8638206720352173\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.7370619177818298\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8880472183227539\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1123733520507812\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.41045343875885\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8221461772918701\n",
            "\n",
            "Total benign train accuarcy: 66.436\n",
            "Total benign train loss: 1582.37661164999\n",
            "\n",
            "[ Test epoch: 50 ]\n",
            "\n",
            "Test accuarcy: 66.13\n",
            "Test average loss: 0.031584022280573845\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 51 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9816141128540039\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.740614652633667\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0619415044784546\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3591262102127075\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1831471920013428\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9871731400489807\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7712970972061157\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6609014868736267\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.132232904434204\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.955497682094574\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8611485958099365\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0923885107040405\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1397833824157715\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9309146404266357\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9219759106636047\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1195690631866455\n",
            "\n",
            "Total benign train accuarcy: 66.364\n",
            "Total benign train loss: 1572.9114724695683\n",
            "\n",
            "[ Test epoch: 51 ]\n",
            "\n",
            "Test accuarcy: 69.15\n",
            "Test average loss: 0.02864294568002224\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 52 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1543841361999512\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9863864779472351\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.018861174583435\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8371530175209045\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9793747067451477\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0143508911132812\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9353898763656616\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9406580328941345\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9925145506858826\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9471239447593689\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9480289816856384\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.3284964561462402\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1499823331832886\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.120171308517456\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1068165302276611\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1244615316390991\n",
            "\n",
            "Total benign train accuarcy: 66.22\n",
            "Total benign train loss: 1582.9711271822453\n",
            "\n",
            "[ Test epoch: 52 ]\n",
            "\n",
            "Test accuarcy: 64.23\n",
            "Test average loss: 0.03324289103150368\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 53 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.1219786405563354\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9515127539634705\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9035505056381226\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1601964235305786\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0092130899429321\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8717754483222961\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9040060043334961\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0604487657546997\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.515231966972351\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.7206341028213501\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1299362182617188\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9933944940567017\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7745686173439026\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9951335191726685\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6180941462516785\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2682331800460815\n",
            "\n",
            "Total benign train accuarcy: 66.242\n",
            "Total benign train loss: 1578.1605986058712\n",
            "\n",
            "[ Test epoch: 53 ]\n",
            "\n",
            "Test accuarcy: 67.63\n",
            "Test average loss: 0.03018998395204544\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 54 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7346142530441284\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7122756838798523\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0648818016052246\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.7644946575164795\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8539110422134399\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6327868103981018\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1091984510421753\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9460780024528503\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6545525193214417\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3275903463363647\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.4375\n",
            "Current benign train loss: 1.8047542572021484\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0890916585922241\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8731476068496704\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.876682460308075\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.01869797706604\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8549791574478149\n",
            "\n",
            "Total benign train accuarcy: 66.55\n",
            "Total benign train loss: 1565.2247578799725\n",
            "\n",
            "[ Test epoch: 54 ]\n",
            "\n",
            "Test accuarcy: 67.59\n",
            "Test average loss: 0.03116500649154186\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 55 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.223747968673706\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7744142413139343\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.04185152053833\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.8873296976089478\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9939616918563843\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.5746910572052002\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1755667924880981\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6622173190116882\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7721875905990601\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7589975595474243\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8354793190956116\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.3749632835388184\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8348851799964905\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1458197832107544\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9000263214111328\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8436993360519409\n",
            "\n",
            "Total benign train accuarcy: 66.3\n",
            "Total benign train loss: 1576.803148508072\n",
            "\n",
            "[ Test epoch: 55 ]\n",
            "\n",
            "Test accuarcy: 66.28\n",
            "Test average loss: 0.03221403047740459\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 56 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7998371720314026\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.26206636428833\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1979455947875977\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.952254056930542\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.0822968482971191\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4229426383972168\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3183672428131104\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0173022747039795\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0781222581863403\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0170751810073853\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7900907397270203\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7782846093177795\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1248677968978882\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.02907133102417\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.7669072151184082\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7668079733848572\n",
            "\n",
            "Total benign train accuarcy: 66.01\n",
            "Total benign train loss: 1576.7467776536942\n",
            "\n",
            "[ Test epoch: 56 ]\n",
            "\n",
            "Test accuarcy: 64.1\n",
            "Test average loss: 0.03418681658506394\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 57 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9410325288772583\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7129604816436768\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8795850872993469\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8335908651351929\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8824767470359802\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9971311688423157\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2330288887023926\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.689616322517395\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8568692207336426\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7219790816307068\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0926308631896973\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2017912864685059\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.807628870010376\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9306832551956177\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1819627285003662\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1151551008224487\n",
            "\n",
            "Total benign train accuarcy: 66.312\n",
            "Total benign train loss: 1575.5987799465656\n",
            "\n",
            "[ Test epoch: 57 ]\n",
            "\n",
            "Test accuarcy: 68.19\n",
            "Test average loss: 0.029715559858083725\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 58 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2114536762237549\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.7039989829063416\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.180321216583252\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0074266195297241\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1941416263580322\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.4130483865737915\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1767936944961548\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2478595972061157\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0126153230667114\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.598980188369751\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9256479740142822\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9459381699562073\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.7436456680297852\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.88434237241745\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 0.9874444603919983\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9018859267234802\n",
            "\n",
            "Total benign train accuarcy: 66.158\n",
            "Total benign train loss: 1580.369440883398\n",
            "\n",
            "[ Test epoch: 58 ]\n",
            "\n",
            "Test accuarcy: 67.05\n",
            "Test average loss: 0.030603662124276163\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 59 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9528467059135437\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.116180658340454\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8381122946739197\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.7540484070777893\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.325644612312317\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7014517784118652\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2046444416046143\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9083563685417175\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8365229368209839\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7044505476951599\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0078355073928833\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6497901082038879\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8447912335395813\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8573350310325623\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8481047749519348\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4897137880325317\n",
            "\n",
            "Total benign train accuarcy: 66.568\n",
            "Total benign train loss: 1566.519556671381\n",
            "\n",
            "[ Test epoch: 59 ]\n",
            "\n",
            "Test accuarcy: 62.89\n",
            "Test average loss: 0.03423460128307342\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 60 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9119968414306641\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8810670971870422\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1411901712417603\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.6727458238601685\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7390587329864502\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2562298774719238\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9993545413017273\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.170850157737732\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9760124683380127\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9834641814231873\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.157845139503479\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0430275201797485\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.839178204536438\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9954933524131775\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0287766456604004\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.45097294449806213\n",
            "\n",
            "Total benign train accuarcy: 66.286\n",
            "Total benign train loss: 1577.5478798151016\n",
            "\n",
            "[ Test epoch: 60 ]\n",
            "\n",
            "Test accuarcy: 69.34\n",
            "Test average loss: 0.028702011281251907\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 61 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1070363521575928\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0315364599227905\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2811439037322998\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.125168800354004\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.6213443279266357\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1257749795913696\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6558958888053894\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.7832386493682861\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2675325870513916\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9992931485176086\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.780879020690918\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8057740330696106\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0096584558486938\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.041240930557251\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.770088791847229\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0334056615829468\n",
            "\n",
            "Total benign train accuarcy: 66.63\n",
            "Total benign train loss: 1561.720834761858\n",
            "\n",
            "[ Test epoch: 61 ]\n",
            "\n",
            "Test accuarcy: 65.73\n",
            "Test average loss: 0.03166551795601845\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 62 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.944517195224762\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0450260639190674\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9685429930686951\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9407638907432556\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1707513332366943\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0002835988998413\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9918800592422485\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.4630118608474731\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7957417964935303\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8917751312255859\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3175991773605347\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3073793649673462\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7479566335678101\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1942251920700073\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0094718933105469\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9012612700462341\n",
            "\n",
            "Total benign train accuarcy: 66.612\n",
            "Total benign train loss: 1567.427939414978\n",
            "\n",
            "[ Test epoch: 62 ]\n",
            "\n",
            "Test accuarcy: 65.34\n",
            "Test average loss: 0.031107170379161834\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 63 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.880035400390625\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7484307885169983\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0018410682678223\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3692889213562012\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8245592713356018\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8692325949668884\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0225496292114258\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1577600240707397\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2814881801605225\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9117428064346313\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9980900883674622\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8736790418624878\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9087715148925781\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9987503886222839\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8253182172775269\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.344435453414917\n",
            "\n",
            "Total benign train accuarcy: 66.472\n",
            "Total benign train loss: 1571.1650571227074\n",
            "\n",
            "[ Test epoch: 63 ]\n",
            "\n",
            "Test accuarcy: 66.55\n",
            "Test average loss: 0.031113127037882805\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 64 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9593958854675293\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0771383047103882\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5794023275375366\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9600582122802734\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7923478484153748\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8178114891052246\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9596180319786072\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.7746274471282959\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9896318912506104\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3077408075332642\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8751810193061829\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8603490591049194\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7475418448448181\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.05352783203125\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6328229904174805\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.4375\n",
            "Current benign train loss: 1.6306029558181763\n",
            "\n",
            "Total benign train accuarcy: 66.446\n",
            "Total benign train loss: 1566.8110150396824\n",
            "\n",
            "[ Test epoch: 64 ]\n",
            "\n",
            "Test accuarcy: 65.37\n",
            "Test average loss: 0.031483275026082995\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 65 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0867855548858643\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9158271551132202\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1564826965332031\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7634039521217346\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9359176158905029\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2376831769943237\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9658559560775757\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.020979881286621\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0190632343292236\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1924848556518555\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8996268510818481\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.417660117149353\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6276693344116211\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8344947099685669\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1453384160995483\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9895995855331421\n",
            "\n",
            "Total benign train accuarcy: 66.636\n",
            "Total benign train loss: 1559.3741815388203\n",
            "\n",
            "[ Test epoch: 65 ]\n",
            "\n",
            "Test accuarcy: 70.6\n",
            "Test average loss: 0.02760383272767067\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 66 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6725633144378662\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8830736875534058\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6982321739196777\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9603595733642578\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.972835123538971\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0549224615097046\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8835707306861877\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9236138463020325\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5179153084754944\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0529873371124268\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2094625234603882\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1530723571777344\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1098353862762451\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2112623453140259\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1141406297683716\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.744286060333252\n",
            "\n",
            "Total benign train accuarcy: 66.91\n",
            "Total benign train loss: 1558.7487918138504\n",
            "\n",
            "[ Test epoch: 66 ]\n",
            "\n",
            "Test accuarcy: 67.43\n",
            "Test average loss: 0.030632467901706695\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 67 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0075864791870117\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0265430212020874\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1621160507202148\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8126670122146606\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7006897926330566\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.921567440032959\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.928482711315155\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0322742462158203\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8397442102432251\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.010056972503662\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8688938021659851\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.84720379114151\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3105043172836304\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9389346241950989\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2207268476486206\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9604303240776062\n",
            "\n",
            "Total benign train accuarcy: 66.71\n",
            "Total benign train loss: 1558.8994075655937\n",
            "\n",
            "[ Test epoch: 67 ]\n",
            "\n",
            "Test accuarcy: 69.96\n",
            "Test average loss: 0.028197239115834237\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 68 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0334912538528442\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7284823656082153\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9892438650131226\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0308758020401\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.742313027381897\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.7968698143959045\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8391249179840088\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7693444490432739\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0094658136367798\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7316505908966064\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0831512212753296\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.301286220550537\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9012348055839539\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9107853770256042\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.3373816013336182\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7711409330368042\n",
            "\n",
            "Total benign train accuarcy: 66.87\n",
            "Total benign train loss: 1555.9178895056248\n",
            "\n",
            "[ Test epoch: 68 ]\n",
            "\n",
            "Test accuarcy: 67.12\n",
            "Test average loss: 0.031872624719142914\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 69 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9883012771606445\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0071059465408325\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9936721324920654\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0759581327438354\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9582775235176086\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8835171461105347\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8129057288169861\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8088585734367371\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 0.9958754777908325\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1832363605499268\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6511504054069519\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9174730777740479\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0901482105255127\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0264250040054321\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2257442474365234\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7519503235816956\n",
            "\n",
            "Total benign train accuarcy: 66.72\n",
            "Total benign train loss: 1562.358128964901\n",
            "\n",
            "[ Test epoch: 69 ]\n",
            "\n",
            "Test accuarcy: 64.87\n",
            "Test average loss: 0.031750834447145465\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 70 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9157010912895203\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.111846685409546\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8848892450332642\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9801284670829773\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8474318981170654\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.16581130027771\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0581746101379395\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8518641591072083\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2978620529174805\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.5329945087432861\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9044069051742554\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 0.9881972670555115\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8631972670555115\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2802224159240723\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.391771674156189\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1227034330368042\n",
            "\n",
            "Total benign train accuarcy: 66.474\n",
            "Total benign train loss: 1568.2916688024998\n",
            "\n",
            "[ Test epoch: 70 ]\n",
            "\n",
            "Test accuarcy: 70.08\n",
            "Test average loss: 0.028646197402477265\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 71 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9674028158187866\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9391943216323853\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9342871308326721\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.3429728746414185\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1328058242797852\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0134750604629517\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.916446328163147\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.6985207796096802\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0053555965423584\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0922185182571411\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.261311411857605\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1983416080474854\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5748969316482544\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0492254495620728\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7072083950042725\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.6838545203208923\n",
            "\n",
            "Total benign train accuarcy: 66.546\n",
            "Total benign train loss: 1564.212094038725\n",
            "\n",
            "[ Test epoch: 71 ]\n",
            "\n",
            "Test accuarcy: 65.45\n",
            "Test average loss: 0.033083218917250634\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 72 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.1225109100341797\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8624598979949951\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3417741060256958\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9116696715354919\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8396693468093872\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.9143002033233643\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1322119235992432\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5903777480125427\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0530178546905518\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7162361741065979\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1186928749084473\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7145838141441345\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1956803798675537\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9796441197395325\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8986297845840454\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9518320560455322\n",
            "\n",
            "Total benign train accuarcy: 66.87\n",
            "Total benign train loss: 1549.216352790594\n",
            "\n",
            "[ Test epoch: 72 ]\n",
            "\n",
            "Test accuarcy: 67.83\n",
            "Test average loss: 0.031388711446523665\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 73 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 0.9855188131332397\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2950831651687622\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.4603691101074219\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7957722544670105\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8671625852584839\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.6069643497467041\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.0995415449142456\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6695578694343567\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1491109132766724\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9515012502670288\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3356492519378662\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7436294555664062\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0188630819320679\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.117235541343689\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0747826099395752\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.358492136001587\n",
            "\n",
            "Total benign train accuarcy: 66.902\n",
            "Total benign train loss: 1550.2094013094902\n",
            "\n",
            "[ Test epoch: 73 ]\n",
            "\n",
            "Test accuarcy: 65.73\n",
            "Test average loss: 0.03230691577196121\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 74 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1538782119750977\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7670769691467285\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8684588670730591\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.79808509349823\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0442984104156494\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8667094111442566\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.531087338924408\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.826564610004425\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1843537092208862\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1074367761611938\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6978577375411987\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9577354788780212\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.90625\n",
            "Current benign train loss: 0.4820292890071869\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.2269532680511475\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1137797832489014\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.032270908355713\n",
            "\n",
            "Total benign train accuarcy: 66.704\n",
            "Total benign train loss: 1562.8020306825638\n",
            "\n",
            "[ Test epoch: 74 ]\n",
            "\n",
            "Test accuarcy: 67.98\n",
            "Test average loss: 0.029956205135583877\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 75 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3099117279052734\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0910831689834595\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2484456300735474\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.6105416417121887\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9348817467689514\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.5997049808502197\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7201070785522461\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.242204189300537\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7097469568252563\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1824226379394531\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9100625514984131\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.6005254983901978\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7737569808959961\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.368836522102356\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.164247751235962\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3252960443496704\n",
            "\n",
            "Total benign train accuarcy: 67.008\n",
            "Total benign train loss: 1553.023219794035\n",
            "\n",
            "[ Test epoch: 75 ]\n",
            "\n",
            "Test accuarcy: 68.97\n",
            "Test average loss: 0.030083403161168098\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 76 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.721623420715332\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0937433242797852\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5202136039733887\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.08919095993042\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1993255615234375\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1827359199523926\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8265935182571411\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9953680038452148\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8510580658912659\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8844950199127197\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.8758403062820435\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0313996076583862\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8602230548858643\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8885703086853027\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.035345196723938\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9837216734886169\n",
            "\n",
            "Total benign train accuarcy: 66.872\n",
            "Total benign train loss: 1553.4436527192593\n",
            "\n",
            "[ Test epoch: 76 ]\n",
            "\n",
            "Test accuarcy: 70.07\n",
            "Test average loss: 0.02825266392529011\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 77 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8635343313217163\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8332397937774658\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1919201612472534\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6526476740837097\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9178460836410522\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1112239360809326\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0719472169876099\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8715052008628845\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.0905286073684692\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0370802879333496\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7100762128829956\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6674426794052124\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0688674449920654\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.1502912044525146\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.999023973941803\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1583127975463867\n",
            "\n",
            "Total benign train accuarcy: 66.734\n",
            "Total benign train loss: 1550.634339183569\n",
            "\n",
            "[ Test epoch: 77 ]\n",
            "\n",
            "Test accuarcy: 60.81\n",
            "Test average loss: 0.0363721676915884\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 78 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3382858037948608\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7101441621780396\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6956168413162231\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.4605188369750977\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7061799764633179\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.021083950996399\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2566617727279663\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5357019305229187\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1868245601654053\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9902684092521667\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8669841885566711\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5519862771034241\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.201483130455017\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5902326703071594\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.419514775276184\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.855401873588562\n",
            "\n",
            "Total benign train accuarcy: 66.808\n",
            "Total benign train loss: 1545.212125480175\n",
            "\n",
            "[ Test epoch: 78 ]\n",
            "\n",
            "Test accuarcy: 65.73\n",
            "Test average loss: 0.03267918421924114\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 79 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9029024839401245\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8067691326141357\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3140579462051392\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1992439031600952\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.903623104095459\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0432624816894531\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.154480218887329\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8478046655654907\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9553709626197815\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8649922013282776\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.310667872428894\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2942463159561157\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9065186977386475\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.090840220451355\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.966564416885376\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.8434187173843384\n",
            "\n",
            "Total benign train accuarcy: 67.154\n",
            "Total benign train loss: 1543.7812151312828\n",
            "\n",
            "[ Test epoch: 79 ]\n",
            "\n",
            "Test accuarcy: 68.45\n",
            "Test average loss: 0.02919573877155781\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 80 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1473716497421265\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.823757529258728\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1188167333602905\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6439926624298096\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9815677404403687\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.6503506898880005\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.229882836341858\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2157477140426636\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0745954513549805\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1638091802597046\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.170734167098999\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.4504374265670776\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8584238290786743\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8099319934844971\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.025138258934021\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.234230399131775\n",
            "\n",
            "Total benign train accuarcy: 66.832\n",
            "Total benign train loss: 1547.0542260110378\n",
            "\n",
            "[ Test epoch: 80 ]\n",
            "\n",
            "Test accuarcy: 64.76\n",
            "Test average loss: 0.03280212311446667\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 81 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0023481845855713\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7055528163909912\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.9441503286361694\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7074123024940491\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1070671081542969\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.822471022605896\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0917623043060303\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.0565918684005737\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.96875\n",
            "Current benign train loss: 0.37828031182289124\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9982170462608337\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.2272511720657349\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1693501472473145\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1293171644210815\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6741604208946228\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.093389630317688\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0065652132034302\n",
            "\n",
            "Total benign train accuarcy: 66.722\n",
            "Total benign train loss: 1558.3858360350132\n",
            "\n",
            "[ Test epoch: 81 ]\n",
            "\n",
            "Test accuarcy: 66.93\n",
            "Test average loss: 0.03105035086274147\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 82 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6565626263618469\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2750645875930786\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8222099542617798\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0470765829086304\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7823605537414551\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 0.9316532015800476\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7287129163742065\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2069467306137085\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0233343839645386\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8566954731941223\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.068583369255066\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1559619903564453\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6331263184547424\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8295416831970215\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8063202500343323\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7267000079154968\n",
            "\n",
            "Total benign train accuarcy: 66.826\n",
            "Total benign train loss: 1558.2160580456257\n",
            "\n",
            "[ Test epoch: 82 ]\n",
            "\n",
            "Test accuarcy: 67.67\n",
            "Test average loss: 0.03066541613340378\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 83 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9606319069862366\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.4353315830230713\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6265132427215576\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6202303767204285\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8423839211463928\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.47162002325057983\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8522791266441345\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.094116449356079\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2715426683425903\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.77283775806427\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.163996934890747\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6434417366981506\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0139849185943604\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1165060997009277\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0861854553222656\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9780959486961365\n",
            "\n",
            "Total benign train accuarcy: 67.064\n",
            "Total benign train loss: 1550.3920267224312\n",
            "\n",
            "[ Test epoch: 83 ]\n",
            "\n",
            "Test accuarcy: 65.68\n",
            "Test average loss: 0.033553065526485445\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 84 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.578050434589386\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6819433569908142\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9916245341300964\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0441529750823975\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8216288089752197\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.8017230033874512\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6743949055671692\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8576416373252869\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1993898153305054\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9428033828735352\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0286173820495605\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.643407940864563\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0629035234451294\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.85621178150177\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9290254712104797\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8756775856018066\n",
            "\n",
            "Total benign train accuarcy: 66.932\n",
            "Total benign train loss: 1541.7154668271542\n",
            "\n",
            "[ Test epoch: 84 ]\n",
            "\n",
            "Test accuarcy: 69.1\n",
            "Test average loss: 0.02866303748190403\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 85 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9179083108901978\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7350192070007324\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9841969013214111\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5726041197776794\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5509483218193054\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7470274567604065\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7658752799034119\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0299097299575806\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9327796101570129\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.047646164894104\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8158184289932251\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1428039073944092\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9569194912910461\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.440791368484497\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3333910703659058\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.52242112159729\n",
            "\n",
            "Total benign train accuarcy: 66.584\n",
            "Total benign train loss: 1558.8064469993114\n",
            "\n",
            "[ Test epoch: 85 ]\n",
            "\n",
            "Test accuarcy: 64.94\n",
            "Test average loss: 0.033134077477455136\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 86 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2889916896820068\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9154139161109924\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8389900326728821\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8485971689224243\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.9341684579849243\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0500528812408447\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0021860599517822\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0075364112854004\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9058858752250671\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0640597343444824\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0685982704162598\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9430291056632996\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8177102208137512\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8811299800872803\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.875449001789093\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3169288635253906\n",
            "\n",
            "Total benign train accuarcy: 67.266\n",
            "Total benign train loss: 1541.0108520388603\n",
            "\n",
            "[ Test epoch: 86 ]\n",
            "\n",
            "Test accuarcy: 69.52\n",
            "Test average loss: 0.02878674022257328\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 87 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8042017221450806\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6859384775161743\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9414616227149963\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5999414324760437\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9420292377471924\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2530847787857056\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9053035378456116\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9832349419593811\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8946748971939087\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2964813709259033\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.558214545249939\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6638307571411133\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.40625\n",
            "Current benign train loss: 1.4073829650878906\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7983863949775696\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.6152851581573486\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9677280187606812\n",
            "\n",
            "Total benign train accuarcy: 67.324\n",
            "Total benign train loss: 1536.2409791052341\n",
            "\n",
            "[ Test epoch: 87 ]\n",
            "\n",
            "Test accuarcy: 66.01\n",
            "Test average loss: 0.0328224096506834\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 88 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0001795291900635\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2943578958511353\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9849267601966858\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.120047688484192\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5875869989395142\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0517855882644653\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1097216606140137\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.133347511291504\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8976095914840698\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6357057094573975\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7904725670814514\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8411654829978943\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9340517520904541\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8209351897239685\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9314142465591431\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.053476095199585\n",
            "\n",
            "Total benign train accuarcy: 67.112\n",
            "Total benign train loss: 1540.210517346859\n",
            "\n",
            "[ Test epoch: 88 ]\n",
            "\n",
            "Test accuarcy: 67.88\n",
            "Test average loss: 0.029184300220012664\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 89 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6524819135665894\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0322898626327515\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7481617331504822\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1655468940734863\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1243302822113037\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7465778589248657\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1169109344482422\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2398769855499268\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1368207931518555\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1270474195480347\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.130544900894165\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8098844289779663\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2636338472366333\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1079963445663452\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6767617464065552\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.9111820459365845\n",
            "\n",
            "Total benign train accuarcy: 67.01\n",
            "Total benign train loss: 1549.546400219202\n",
            "\n",
            "[ Test epoch: 89 ]\n",
            "\n",
            "Test accuarcy: 70.25\n",
            "Test average loss: 0.02757337909936905\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 90 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9456093311309814\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7822349667549133\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.4589971899986267\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0409024953842163\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.432144045829773\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0268670320510864\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.2445801496505737\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0169907808303833\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.962953507900238\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6352876424789429\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3306735754013062\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1420706510543823\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8930922746658325\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.91484135389328\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0871226787567139\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9416711926460266\n",
            "\n",
            "Total benign train accuarcy: 67.444\n",
            "Total benign train loss: 1533.8956432640553\n",
            "\n",
            "[ Test epoch: 90 ]\n",
            "\n",
            "Test accuarcy: 64.15\n",
            "Test average loss: 0.03394235211610794\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 91 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.1513079404830933\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 0.9103640913963318\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0639370679855347\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.357161521911621\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9341184496879578\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8445808291435242\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9122552871704102\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9107922315597534\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2746179103851318\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.627898633480072\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8108746409416199\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8026215434074402\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9554341435432434\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0011473894119263\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9620141983032227\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8874691128730774\n",
            "\n",
            "Total benign train accuarcy: 67.268\n",
            "Total benign train loss: 1533.8221283853054\n",
            "\n",
            "[ Test epoch: 91 ]\n",
            "\n",
            "Test accuarcy: 67.08\n",
            "Test average loss: 0.03211858998835087\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 92 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8734216690063477\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9013301134109497\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9792443513870239\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0004135370254517\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1253401041030884\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2180132865905762\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 0.9635816812515259\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8007794618606567\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2800755500793457\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8632963299751282\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9238088130950928\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7753126621246338\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7159430384635925\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2239861488342285\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.854034960269928\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7586721777915955\n",
            "\n",
            "Total benign train accuarcy: 67.18\n",
            "Total benign train loss: 1543.9527520239353\n",
            "\n",
            "[ Test epoch: 92 ]\n",
            "\n",
            "Test accuarcy: 71.59\n",
            "Test average loss: 0.026241192665696144\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 93 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0090242624282837\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0686607360839844\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9200451970100403\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.13089919090271\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0136330127716064\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6616712808609009\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6601972579956055\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8779038786888123\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1502742767333984\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9598257541656494\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8654457926750183\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6959794759750366\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6674608588218689\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6320452690124512\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.006733775138855\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1121584177017212\n",
            "\n",
            "Total benign train accuarcy: 67.524\n",
            "Total benign train loss: 1530.7361044585705\n",
            "\n",
            "[ Test epoch: 93 ]\n",
            "\n",
            "Test accuarcy: 69.7\n",
            "Test average loss: 0.02742902475297451\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 94 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0061118602752686\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3368970155715942\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0799766778945923\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8344772458076477\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9859485030174255\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3965195417404175\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.00527822971344\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0006098747253418\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1946998834609985\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7823512554168701\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2703611850738525\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2089650630950928\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0094008445739746\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9814689755439758\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 2.1249873638153076\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0066018104553223\n",
            "\n",
            "Total benign train accuarcy: 66.832\n",
            "Total benign train loss: 1542.6273405253887\n",
            "\n",
            "[ Test epoch: 94 ]\n",
            "\n",
            "Test accuarcy: 69.66\n",
            "Test average loss: 0.028494741773605347\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 95 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2543102502822876\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9817458987236023\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0612826347351074\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9079192876815796\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6886928081512451\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6264781951904297\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2431857585906982\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2218612432479858\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6581805348396301\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8891334533691406\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.944934070110321\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7689520120620728\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.5511225461959839\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0635738372802734\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7415440678596497\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9458362460136414\n",
            "\n",
            "Total benign train accuarcy: 67.302\n",
            "Total benign train loss: 1525.349168151617\n",
            "\n",
            "[ Test epoch: 95 ]\n",
            "\n",
            "Test accuarcy: 69.11\n",
            "Test average loss: 0.028380817154049873\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 96 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7755380272865295\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.887750506401062\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5882189869880676\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6358394026756287\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.682839572429657\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3448032140731812\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0309275388717651\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7946276068687439\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.3886784315109253\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0462723970413208\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3675540685653687\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9337093830108643\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2383203506469727\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1556504964828491\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8188933730125427\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2154860496520996\n",
            "\n",
            "Total benign train accuarcy: 67.042\n",
            "Total benign train loss: 1538.6519081294537\n",
            "\n",
            "[ Test epoch: 96 ]\n",
            "\n",
            "Test accuarcy: 65.25\n",
            "Test average loss: 0.033745599681138994\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 97 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.075673222541809\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9510717391967773\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9302636981010437\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1514813899993896\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1717181205749512\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9689123630523682\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0473438501358032\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0292032957077026\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0229413509368896\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2106528282165527\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.5638691186904907\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.4488611221313477\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.6643263101577759\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0741866827011108\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0544952154159546\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7986956834793091\n",
            "\n",
            "Total benign train accuarcy: 67.438\n",
            "Total benign train loss: 1533.4040311574936\n",
            "\n",
            "[ Test epoch: 97 ]\n",
            "\n",
            "Test accuarcy: 67.4\n",
            "Test average loss: 0.030479284757375717\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 98 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0559303760528564\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0507949590682983\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.143423080444336\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0350977182388306\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6851320266723633\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.925448477268219\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1186389923095703\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.6812068223953247\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2260072231292725\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9860254526138306\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3168609142303467\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7455698847770691\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8556793332099915\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6593521237373352\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8192164897918701\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5885920524597168\n",
            "\n",
            "Total benign train accuarcy: 67.122\n",
            "Total benign train loss: 1546.4578143954277\n",
            "\n",
            "[ Test epoch: 98 ]\n",
            "\n",
            "Test accuarcy: 67.44\n",
            "Test average loss: 0.030393870505690576\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 99 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2046473026275635\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.4635165929794312\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3284931182861328\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8724544048309326\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.507674515247345\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8992455005645752\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9868077635765076\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1106942892074585\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1197539567947388\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.4988214075565338\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.7613986730575562\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9287781715393066\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.430567741394043\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1885967254638672\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7013589143753052\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0447431802749634\n",
            "\n",
            "Total benign train accuarcy: 67.136\n",
            "Total benign train loss: 1541.567043542862\n",
            "\n",
            "[ Test epoch: 99 ]\n",
            "\n",
            "Test accuarcy: 68.54\n",
            "Test average loss: 0.02927547116279602\n",
            "Model Saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Plotting Train Accuracy & Loss"
      ],
      "metadata": {
        "id": "gLQzz8nykn2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch =[]\n",
        "\n",
        "for i , loss in enumerate(log_test_total_loss):\n",
        "  epoch.append(i+1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('log_train_accuracy')\n",
        "plt.plot(epoch , log_train_total_accuracy)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('train_accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "sQd8X8c8iGw8",
        "outputId": "4070bb65-771d-48c2-c391-4666605fd131"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('log_train_loss')\n",
        "plt.plot(epoch , log_train_total_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('train_loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "_oxOu_FeiMyB",
        "outputId": "79793f0b-6c35-4da2-f436-7bf7e578e14f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Plotting Test Accuracy & Loss"
      ],
      "metadata": {
        "id": "u-D79-mzkuML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('log_test_accuracy')\n",
        "plt.plot(epoch , log_test_total_accuracy)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('test_accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "vygvxsl6iOpl",
        "outputId": "c3049ffa-ad6a-49c2-b4ca-50b4be4d206f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('log_test_loss')\n",
        "plt.plot(epoch , log_test_total_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('test_loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "H8h-hfekiRDo",
        "outputId": "964f74ba-e8a6-46aa-bff2-6b686b597fa9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Print Result"
      ],
      "metadata": {
        "id": "JkGVx8krkx4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(log_train_total_accuracy)\n",
        "print(log_train_total_loss) \n",
        "print(log_test_total_accuracy)\n",
        "print(log_test_total_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXoGTukRiTpF",
        "outputId": "7506ea43-f394-4edf-c490-2be177e103ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13.874, 20.562, 26.506, 33.11, 36.516, 38.63, 39.486, 44.332, 47.838, 50.578, 53.356, 55.25, 56.742, 57.992, 58.988, 59.448, 59.988, 60.98, 61.498, 61.848, 62.318, 62.582, 62.926, 62.988, 63.166, 64.012, 63.922, 63.79, 64.248, 64.122, 64.696, 64.558, 64.598, 64.63, 64.954, 65.29, 65.198, 65.366, 65.418, 65.296, 65.306, 65.532, 65.616, 65.54, 65.698, 65.982, 65.938, 65.73, 65.99, 66.1, 66.436, 66.364, 66.22, 66.242, 66.55, 66.3, 66.01, 66.312, 66.158, 66.568, 66.286, 66.63, 66.612, 66.472, 66.446, 66.636, 66.91, 66.71, 66.87, 66.72, 66.474, 66.546, 66.87, 66.902, 66.704, 67.008, 66.872, 66.734, 66.808, 67.154, 66.832, 66.722, 66.826, 67.064, 66.932, 66.584, 67.266, 67.324, 67.112, 67.01, 67.444, 67.268, 67.18, 67.524, 66.832, 67.302, 67.042, 67.438, 67.122, 67.136]\n",
            "[8365.783673763275, 3259.3487960100174, 3050.54653775692, 2792.7285112142563, 2677.903884410858, 2604.095287799835, 2575.5311648845673, 2396.0522525906563, 2281.445233464241, 2195.3171661496162, 2077.8700806498528, 2004.0515367388725, 1943.6100969910622, 1889.4206160902977, 1856.563063621521, 1832.6485384106636, 1821.6569944024086, 1784.508464396, 1764.804499566555, 1756.8667741417885, 1732.0832035541534, 1720.7182210087776, 1703.6033006310463, 1694.731696665287, 1709.44222843647, 1669.1497807502747, 1670.8008356690407, 1677.1335924565792, 1648.9795221090317, 1659.392717719078, 1635.2159690260887, 1639.7532708346844, 1627.9994948506355, 1628.9436132013798, 1634.937134951353, 1615.5917722284794, 1624.3486448526382, 1617.4029343128204, 1606.7527054548264, 1614.1037211418152, 1619.2214652001858, 1599.2726356685162, 1610.3486645519733, 1606.680177628994, 1600.43694755435, 1594.2839897572994, 1587.8543565273285, 1598.0760312080383, 1588.1661405563354, 1574.685079574585, 1582.37661164999, 1572.9114724695683, 1582.9711271822453, 1578.1605986058712, 1565.2247578799725, 1576.803148508072, 1576.7467776536942, 1575.5987799465656, 1580.369440883398, 1566.519556671381, 1577.5478798151016, 1561.720834761858, 1567.427939414978, 1571.1650571227074, 1566.8110150396824, 1559.3741815388203, 1558.7487918138504, 1558.8994075655937, 1555.9178895056248, 1562.358128964901, 1568.2916688024998, 1564.212094038725, 1549.216352790594, 1550.2094013094902, 1562.8020306825638, 1553.023219794035, 1553.4436527192593, 1550.634339183569, 1545.212125480175, 1543.7812151312828, 1547.0542260110378, 1558.3858360350132, 1558.2160580456257, 1550.3920267224312, 1541.7154668271542, 1558.8064469993114, 1541.0108520388603, 1536.2409791052341, 1540.210517346859, 1549.546400219202, 1533.8956432640553, 1533.8221283853054, 1543.9527520239353, 1530.7361044585705, 1542.6273405253887, 1525.349168151617, 1538.6519081294537, 1533.4040311574936, 1546.4578143954277, 1541.567043542862]\n",
            "[15.6, 24.56, 35.04, 38.09, 36.16, 41.19, 45.14, 46.66, 47.44, 56.72, 46.5, 55.29, 50.72, 60.17, 61.89, 62.45, 59.97, 62.25, 61.83, 54.7, 66.15, 60.64, 63.26, 61.06, 68.0, 69.38, 65.18, 65.39, 67.38, 67.14, 62.24, 66.92, 69.28, 60.24, 64.72, 68.65, 67.76, 66.14, 63.57, 63.35, 60.45, 65.66, 67.59, 67.9, 67.15, 65.11, 69.36, 67.95, 69.73, 64.06, 66.13, 69.15, 64.23, 67.63, 67.59, 66.28, 64.1, 68.19, 67.05, 62.89, 69.34, 65.73, 65.34, 66.55, 65.37, 70.6, 67.43, 69.96, 67.12, 64.87, 70.08, 65.45, 67.83, 65.73, 67.98, 68.97, 70.07, 60.81, 65.73, 68.45, 64.76, 66.93, 67.67, 65.68, 69.1, 64.94, 69.52, 66.01, 67.88, 70.25, 64.15, 67.08, 71.59, 69.7, 69.66, 69.11, 65.25, 67.4, 67.44, 68.54]\n",
            "[0.10671776905059814, 0.0966696364402771, 0.06060177625417709, 0.06627608598470688, 0.059166263580322266, 0.05133385637998581, 0.04678309364318848, 0.04524816384911537, 0.04827517539262772, 0.039141687309741977, 0.0527716630756855, 0.04115006909370422, 0.04632848390340805, 0.037144501477479934, 0.03439647864699364, 0.035668272855877876, 0.03696059748530388, 0.033284826937317845, 0.03432868310213089, 0.04218349732756615, 0.03159478562772274, 0.03723638371229172, 0.0338175280213356, 0.0364395734667778, 0.02930800848007202, 0.02839253209531307, 0.032269510531425474, 0.03216506513059139, 0.030178523260354997, 0.030047561827301977, 0.03561088256239891, 0.029954117900133134, 0.027885147079825402, 0.03888827961087227, 0.03237849652469158, 0.029876072803139687, 0.02906727485060692, 0.03119462400972843, 0.03474110432267189, 0.03576869702339172, 0.03846890930831432, 0.03134393725991249, 0.029896009716391562, 0.030270370319485665, 0.03118058749139309, 0.03356861881613731, 0.028944093862175942, 0.030056731697916984, 0.028625020840764046, 0.033774659115076065, 0.031584022280573845, 0.02864294568002224, 0.03324289103150368, 0.03018998395204544, 0.03116500649154186, 0.03221403047740459, 0.03418681658506394, 0.029715559858083725, 0.030603662124276163, 0.03423460128307342, 0.028702011281251907, 0.03166551795601845, 0.031107170379161834, 0.031113127037882805, 0.031483275026082995, 0.02760383272767067, 0.030632467901706695, 0.028197239115834237, 0.031872624719142914, 0.031750834447145465, 0.028646197402477265, 0.033083218917250634, 0.031388711446523665, 0.03230691577196121, 0.029956205135583877, 0.030083403161168098, 0.02825266392529011, 0.0363721676915884, 0.03267918421924114, 0.02919573877155781, 0.03280212311446667, 0.03105035086274147, 0.03066541613340378, 0.033553065526485445, 0.02866303748190403, 0.033134077477455136, 0.02878674022257328, 0.0328224096506834, 0.029184300220012664, 0.02757337909936905, 0.03394235211610794, 0.03211858998835087, 0.026241192665696144, 0.02742902475297451, 0.028494741773605347, 0.028380817154049873, 0.033745599681138994, 0.030479284757375717, 0.030393870505690576, 0.02927547116279602]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Model Summary"
      ],
      "metadata": {
        "id": "IXxT_BbXk1ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = resnet101(3, 10)\n",
        "summary(model.cuda(), (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMm3jA2WiWHd",
        "outputId": "ae0161c1-8c56-41a9-ca39-41b60a765286"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5            [-1, 256, 8, 8]          16,384\n",
            "       BatchNorm2d-6            [-1, 256, 8, 8]             512\n",
            "        Conv2dAuto-7             [-1, 64, 8, 8]           4,096\n",
            "       BatchNorm2d-8             [-1, 64, 8, 8]             128\n",
            "              ReLU-9             [-1, 64, 8, 8]               0\n",
            "       Conv2dAuto-10             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-11             [-1, 64, 8, 8]             128\n",
            "             ReLU-12             [-1, 64, 8, 8]               0\n",
            "       Conv2dAuto-13            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
            "ResNetBottleNeckBlock-15            [-1, 256, 8, 8]               0\n",
            "       Conv2dAuto-16             [-1, 64, 8, 8]          16,384\n",
            "      BatchNorm2d-17             [-1, 64, 8, 8]             128\n",
            "             ReLU-18             [-1, 64, 8, 8]               0\n",
            "       Conv2dAuto-19             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-20             [-1, 64, 8, 8]             128\n",
            "             ReLU-21             [-1, 64, 8, 8]               0\n",
            "       Conv2dAuto-22            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-23            [-1, 256, 8, 8]             512\n",
            "ResNetBottleNeckBlock-24            [-1, 256, 8, 8]               0\n",
            "       Conv2dAuto-25             [-1, 64, 8, 8]          16,384\n",
            "      BatchNorm2d-26             [-1, 64, 8, 8]             128\n",
            "             ReLU-27             [-1, 64, 8, 8]               0\n",
            "       Conv2dAuto-28             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-29             [-1, 64, 8, 8]             128\n",
            "             ReLU-30             [-1, 64, 8, 8]               0\n",
            "       Conv2dAuto-31            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-32            [-1, 256, 8, 8]             512\n",
            "ResNetBottleNeckBlock-33            [-1, 256, 8, 8]               0\n",
            "      ResNetLayer-34            [-1, 256, 8, 8]               0\n",
            "           Conv2d-35            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-36            [-1, 512, 4, 4]           1,024\n",
            "       Conv2dAuto-37            [-1, 128, 8, 8]          32,768\n",
            "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
            "             ReLU-39            [-1, 128, 8, 8]               0\n",
            "       Conv2dAuto-40            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
            "             ReLU-42            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-43            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
            "ResNetBottleNeckBlock-45            [-1, 512, 4, 4]               0\n",
            "       Conv2dAuto-46            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-47            [-1, 128, 4, 4]             256\n",
            "             ReLU-48            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-49            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
            "             ReLU-51            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-52            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-53            [-1, 512, 4, 4]           1,024\n",
            "ResNetBottleNeckBlock-54            [-1, 512, 4, 4]               0\n",
            "       Conv2dAuto-55            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-56            [-1, 128, 4, 4]             256\n",
            "             ReLU-57            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-58            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-59            [-1, 128, 4, 4]             256\n",
            "             ReLU-60            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-61            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-62            [-1, 512, 4, 4]           1,024\n",
            "ResNetBottleNeckBlock-63            [-1, 512, 4, 4]               0\n",
            "       Conv2dAuto-64            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-65            [-1, 128, 4, 4]             256\n",
            "             ReLU-66            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-67            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-68            [-1, 128, 4, 4]             256\n",
            "             ReLU-69            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-70            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-71            [-1, 512, 4, 4]           1,024\n",
            "ResNetBottleNeckBlock-72            [-1, 512, 4, 4]               0\n",
            "      ResNetLayer-73            [-1, 512, 4, 4]               0\n",
            "           Conv2d-74           [-1, 1024, 2, 2]         524,288\n",
            "      BatchNorm2d-75           [-1, 1024, 2, 2]           2,048\n",
            "       Conv2dAuto-76            [-1, 256, 4, 4]         131,072\n",
            "      BatchNorm2d-77            [-1, 256, 4, 4]             512\n",
            "             ReLU-78            [-1, 256, 4, 4]               0\n",
            "       Conv2dAuto-79            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-80            [-1, 256, 2, 2]             512\n",
            "             ReLU-81            [-1, 256, 2, 2]               0\n",
            "       Conv2dAuto-82           [-1, 1024, 2, 2]         262,144\n",
            "      BatchNorm2d-83           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-84           [-1, 1024, 2, 2]               0\n",
            "       Conv2dAuto-85            [-1, 256, 2, 2]         262,144\n",
            "      BatchNorm2d-86            [-1, 256, 2, 2]             512\n",
            "             ReLU-87            [-1, 256, 2, 2]               0\n",
            "       Conv2dAuto-88            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-89            [-1, 256, 2, 2]             512\n",
            "             ReLU-90            [-1, 256, 2, 2]               0\n",
            "       Conv2dAuto-91           [-1, 1024, 2, 2]         262,144\n",
            "      BatchNorm2d-92           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-93           [-1, 1024, 2, 2]               0\n",
            "       Conv2dAuto-94            [-1, 256, 2, 2]         262,144\n",
            "      BatchNorm2d-95            [-1, 256, 2, 2]             512\n",
            "             ReLU-96            [-1, 256, 2, 2]               0\n",
            "       Conv2dAuto-97            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-98            [-1, 256, 2, 2]             512\n",
            "             ReLU-99            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-100           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-101           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-102           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-103            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-104            [-1, 256, 2, 2]             512\n",
            "            ReLU-105            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-106            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-107            [-1, 256, 2, 2]             512\n",
            "            ReLU-108            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-109           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-110           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-111           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-112            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-113            [-1, 256, 2, 2]             512\n",
            "            ReLU-114            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-115            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-116            [-1, 256, 2, 2]             512\n",
            "            ReLU-117            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-118           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-119           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-120           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-121            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-122            [-1, 256, 2, 2]             512\n",
            "            ReLU-123            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-124            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-125            [-1, 256, 2, 2]             512\n",
            "            ReLU-126            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-127           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-129           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-130            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-131            [-1, 256, 2, 2]             512\n",
            "            ReLU-132            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-133            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-134            [-1, 256, 2, 2]             512\n",
            "            ReLU-135            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-136           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-137           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-138           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-139            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-140            [-1, 256, 2, 2]             512\n",
            "            ReLU-141            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-142            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-143            [-1, 256, 2, 2]             512\n",
            "            ReLU-144            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-145           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-146           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-147           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-148            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-149            [-1, 256, 2, 2]             512\n",
            "            ReLU-150            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-151            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-152            [-1, 256, 2, 2]             512\n",
            "            ReLU-153            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-154           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-155           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-156           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-157            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-158            [-1, 256, 2, 2]             512\n",
            "            ReLU-159            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-160            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-161            [-1, 256, 2, 2]             512\n",
            "            ReLU-162            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-163           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-164           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-165           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-166            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-167            [-1, 256, 2, 2]             512\n",
            "            ReLU-168            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-169            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-170            [-1, 256, 2, 2]             512\n",
            "            ReLU-171            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-172           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-173           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-174           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-175            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-176            [-1, 256, 2, 2]             512\n",
            "            ReLU-177            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-178            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-179            [-1, 256, 2, 2]             512\n",
            "            ReLU-180            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-181           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-182           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-183           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-184            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-185            [-1, 256, 2, 2]             512\n",
            "            ReLU-186            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-187            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-188            [-1, 256, 2, 2]             512\n",
            "            ReLU-189            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-190           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-191           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-192           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-193            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-194            [-1, 256, 2, 2]             512\n",
            "            ReLU-195            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-196            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-197            [-1, 256, 2, 2]             512\n",
            "            ReLU-198            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-199           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-200           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-201           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-202            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-203            [-1, 256, 2, 2]             512\n",
            "            ReLU-204            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-205            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-206            [-1, 256, 2, 2]             512\n",
            "            ReLU-207            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-208           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-209           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-210           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-211            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-212            [-1, 256, 2, 2]             512\n",
            "            ReLU-213            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-214            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-215            [-1, 256, 2, 2]             512\n",
            "            ReLU-216            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-217           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-218           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-219           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-220            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-221            [-1, 256, 2, 2]             512\n",
            "            ReLU-222            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-223            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-224            [-1, 256, 2, 2]             512\n",
            "            ReLU-225            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-226           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-227           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-228           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-229            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-230            [-1, 256, 2, 2]             512\n",
            "            ReLU-231            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-232            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-233            [-1, 256, 2, 2]             512\n",
            "            ReLU-234            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-235           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-236           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-237           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-238            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-239            [-1, 256, 2, 2]             512\n",
            "            ReLU-240            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-241            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-242            [-1, 256, 2, 2]             512\n",
            "            ReLU-243            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-244           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-245           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-246           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-247            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-248            [-1, 256, 2, 2]             512\n",
            "            ReLU-249            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-250            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-251            [-1, 256, 2, 2]             512\n",
            "            ReLU-252            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-253           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-254           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-255           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-256            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-257            [-1, 256, 2, 2]             512\n",
            "            ReLU-258            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-259            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-260            [-1, 256, 2, 2]             512\n",
            "            ReLU-261            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-262           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-263           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-264           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-265            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-266            [-1, 256, 2, 2]             512\n",
            "            ReLU-267            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-268            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-269            [-1, 256, 2, 2]             512\n",
            "            ReLU-270            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-271           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-272           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-273           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-274            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-275            [-1, 256, 2, 2]             512\n",
            "            ReLU-276            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-277            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-278            [-1, 256, 2, 2]             512\n",
            "            ReLU-279            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-280           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-281           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-282           [-1, 1024, 2, 2]               0\n",
            "     ResNetLayer-283           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-284           [-1, 2048, 1, 1]       2,097,152\n",
            "     BatchNorm2d-285           [-1, 2048, 1, 1]           4,096\n",
            "      Conv2dAuto-286            [-1, 512, 2, 2]         524,288\n",
            "     BatchNorm2d-287            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-288            [-1, 512, 2, 2]               0\n",
            "      Conv2dAuto-289            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-290            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-291            [-1, 512, 1, 1]               0\n",
            "      Conv2dAuto-292           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-293           [-1, 2048, 1, 1]           4,096\n",
            "ResNetBottleNeckBlock-294           [-1, 2048, 1, 1]               0\n",
            "      Conv2dAuto-295            [-1, 512, 1, 1]       1,048,576\n",
            "     BatchNorm2d-296            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-297            [-1, 512, 1, 1]               0\n",
            "      Conv2dAuto-298            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-299            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-300            [-1, 512, 1, 1]               0\n",
            "      Conv2dAuto-301           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-302           [-1, 2048, 1, 1]           4,096\n",
            "ResNetBottleNeckBlock-303           [-1, 2048, 1, 1]               0\n",
            "      Conv2dAuto-304            [-1, 512, 1, 1]       1,048,576\n",
            "     BatchNorm2d-305            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-306            [-1, 512, 1, 1]               0\n",
            "      Conv2dAuto-307            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-308            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-309            [-1, 512, 1, 1]               0\n",
            "      Conv2dAuto-310           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-311           [-1, 2048, 1, 1]           4,096\n",
            "ResNetBottleNeckBlock-312           [-1, 2048, 1, 1]               0\n",
            "     ResNetLayer-313           [-1, 2048, 1, 1]               0\n",
            "   ResNetEncoder-314           [-1, 2048, 1, 1]               0\n",
            "AdaptiveAvgPool2d-315           [-1, 2048, 1, 1]               0\n",
            "          Linear-316                   [-1, 10]          20,490\n",
            "   ResnetDecoder-317                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 42,520,650\n",
            "Trainable params: 42,520,650\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 7.64\n",
            "Params size (MB): 162.20\n",
            "Estimated Total Size (MB): 169.86\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qoWe9aVHiY48"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}