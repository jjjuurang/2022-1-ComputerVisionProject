{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTGHKmVakOlS"
      },
      "source": [
        "ResNet Training (CIFAR - 10)\n",
        "1. Import Library & Define Resnet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO62BlwchvVY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class Conv2dAuto(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
        "        \n",
        "\n",
        "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels, self.out_channels =  in_channels, out_channels\n",
        "        self.blocks = nn.Identity()\n",
        "        self.shortcut = nn.Identity()   \n",
        "    \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
        "        x = self.blocks(x)\n",
        "        x += residual\n",
        "        return x\n",
        "    \n",
        "    @property\n",
        "    def should_apply_shortcut(self):\n",
        "        return self.in_channels != self.out_channels\n",
        "    \n",
        "\n",
        "\n",
        "class ResNetResidualBlock(ResidualBlock):\n",
        "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels)\n",
        "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
        "        self.shortcut = nn.Sequential(OrderedDict(\n",
        "        {\n",
        "            'conv' : nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1,\n",
        "                      stride=self.downsampling, bias=False),\n",
        "            'bn' : nn.BatchNorm2d(self.expanded_channels)\n",
        "            \n",
        "        })) if self.should_apply_shortcut else None\n",
        "        \n",
        "        \n",
        "    @property\n",
        "    def expanded_channels(self):\n",
        "        return self.out_channels * self.expansion\n",
        "    \n",
        "    @property\n",
        "    def should_apply_shortcut(self):\n",
        "        return self.in_channels != self.expanded_channels\n",
        "\n",
        "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
        "    return nn.Sequential(OrderedDict({'conv': conv(in_channels, out_channels, *args, **kwargs), \n",
        "                          'bn': nn.BatchNorm2d(out_channels) }))\n",
        "\n",
        "class ResNetBasicBlock(ResNetResidualBlock):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, activation=nn.ReLU, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
        "        self.blocks = nn.Sequential(\n",
        "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
        "            activation(),\n",
        "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
        "        )\n",
        "        \n",
        "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, activation=nn.ReLU, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
        "        self.blocks = nn.Sequential(\n",
        "           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
        "             activation(),\n",
        "             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
        "             activation(),\n",
        "             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
        "        )\n",
        "        \n",
        "class ResNetLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
        "        downsampling = 2 if in_channels != out_channels else 1\n",
        "        \n",
        "        self.blocks = nn.Sequential(\n",
        "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
        "            *[block(out_channels * block.expansion, \n",
        "                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        return x\n",
        "    \n",
        "class ResNetEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet encoder composed by increasing different layers with increasing features.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, blocks_sizes=[64, 128, 256, 512], deepths=[2,2,2,2], \n",
        "                 activation=nn.ReLU, block=ResNetBasicBlock, *args,**kwargs):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.blocks_sizes = blocks_sizes\n",
        "        \n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.blocks_sizes[0], kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(self.blocks_sizes[0]),\n",
        "            activation(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
        "        self.blocks = nn.ModuleList([ \n",
        "            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=deepths[0], activation=activation, \n",
        "                        block=block,  *args, **kwargs),\n",
        "            *[ResNetLayer(in_channels * block.expansion, \n",
        "                          out_channels, n=n, activation=activation, \n",
        "                          block=block, *args, **kwargs) \n",
        "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]       \n",
        "        ])\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.gate(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class ResnetDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents the tail of ResNet. It performs a global pooling and maps the output to the\n",
        "    correct class by using a fully connected layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, n_classes):\n",
        "        super().__init__()\n",
        "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.decoder = nn.Linear(in_features, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "    \n",
        "class ResNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, n_classes, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.encoder = ResNetEncoder(in_channels, *args, **kwargs)\n",
        "        self.decoder = ResnetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "    \n",
        "def resnet18(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBasicBlock, deepths=[2, 2, 2, 2])\n",
        "\n",
        "def resnet34(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBasicBlock, deepths=[3, 4, 6, 3])\n",
        "\n",
        "def resnet50(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBottleNeckBlock, deepths=[3, 4, 6, 3])\n",
        "\n",
        "def resnet101(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBottleNeckBlock, deepths=[3, 4, 23, 3])\n",
        "\n",
        "def resnet152(in_channels, n_classes):\n",
        "    return ResNet(in_channels, n_classes, block=ResNetBottleNeckBlock, deepths=[3, 8, 36, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peNdZNJrkVjd"
      },
      "source": [
        "2. Download CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "ec1a59b962244b6ea398e3e576f6f31b",
            "2c7a1210cd454d728c61a626beb9e9a9",
            "c2cccbe4fe8d4713a20d8bdb4ae19a4c",
            "9ea1753c12c44df29cde6112332dc8ed",
            "f1b5669946a94f058c71acbc697448a0",
            "ef097fdb2e8a489289f347b025115840",
            "56111eb22e2d41b5b48525b2c18b4aa7",
            "8910f5787e904691b0ad378975376e0a",
            "e2a8b53a87f64ac09d40ee92446de645",
            "347e39364e1c460cae53cf331422910d",
            "61569ac89ad441979dde9d62629c53a7"
          ]
        },
        "id": "Gie-we8Th8iX",
        "outputId": "a05733d5-8d89-4b05-f127-98094559c081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec1a59b962244b6ea398e3e576f6f31b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#Download CIFAR10 DATASET\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "\n",
        "    transforms.ToTensor(), normalize\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(), normalize\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLG1W3k6kZwT"
      },
      "source": [
        "3. CUDA setting & Define Train,Test function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxT1pfXwiAGD"
      },
      "outputs": [],
      "source": [
        "#gpu setting & define Training function\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "net = resnet152(3,10)           #resnet101 선언\n",
        "net = net.to(device)\n",
        "net = torch.nn.DataParallel(net)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "learning_rate = 0.1\n",
        "file_name = 'resnet101_cifar10.pt'\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\n[ Train epoch: %d ]' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        benign_outputs = net(inputs)\n",
        "        loss = criterion(benign_outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = benign_outputs.max(1)\n",
        "\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('\\nCurrent batch:', str(batch_idx))\n",
        "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
        "            print('Current benign train loss:', loss.item())\n",
        "\n",
        "    print('\\nTotal benign train accuarcy:', 100. * correct / total)\n",
        "    print('Total benign train loss:', train_loss)\n",
        "    total_accuracy = 100. * correct / total;\n",
        "\n",
        "    return total_accuracy , train_loss\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    print('\\n[ Test epoch: %d ]' % epoch)\n",
        "    net.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print('\\nTest accuarcy:', 100. * correct / total)\n",
        "    print('Test average loss:', loss / total)\n",
        "    total_accuracy = 100. * correct / total;\n",
        "    test_loss = loss / total\n",
        "\n",
        "    state = {\n",
        "        'net': net.state_dict()\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/' + file_name)\n",
        "    print('Model Saved!')\n",
        "\n",
        "    return total_accuracy , test_loss\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lr = learning_rate\n",
        "    if epoch >= 100:\n",
        "        lr /= 10\n",
        "    if epoch >= 150:\n",
        "        lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_duH55imkhMu"
      },
      "source": [
        "4. Run Training & Save log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7vBS8DEiEBD",
        "outputId": "79d49b58-752f-4ce1-a658-1637cef498fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Current benign train loss: 1.1602270603179932\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9491401314735413\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2365728616714478\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.1236518621444702\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8512685894966125\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0165177583694458\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8984228372573853\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.265110731124878\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8071189522743225\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5393834114074707\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5932204127311707\n",
            "\n",
            "Total benign train accuarcy: 65.414\n",
            "Total benign train loss: 1609.9155994057655\n",
            "\n",
            "[ Test epoch: 33 ]\n",
            "\n",
            "Test accuarcy: 62.64\n",
            "Test average loss: 0.03362874470949173\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 34 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.9559633731842041\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7109028100967407\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8909615278244019\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1969633102416992\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0642168521881104\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8669184446334839\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3161358833312988\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7382758855819702\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1807557344436646\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0377111434936523\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.7525861263275146\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7833349108695984\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.022059440612793\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8149924874305725\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.044985294342041\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.331301212310791\n",
            "\n",
            "Total benign train accuarcy: 65.318\n",
            "Total benign train loss: 1608.6464525461197\n",
            "\n",
            "[ Test epoch: 34 ]\n",
            "\n",
            "Test accuarcy: 65.08\n",
            "Test average loss: 0.03294923908114433\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 35 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0607941150665283\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1318002939224243\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8101581335067749\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0639663934707642\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7692273855209351\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6427125334739685\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.467475414276123\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1953680515289307\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7598630785942078\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.389930009841919\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.318777322769165\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9383963942527771\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1184848546981812\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0459973812103271\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7923756241798401\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0880640745162964\n",
            "\n",
            "Total benign train accuarcy: 65.694\n",
            "Total benign train loss: 1593.701498657465\n",
            "\n",
            "[ Test epoch: 35 ]\n",
            "\n",
            "Test accuarcy: 64.8\n",
            "Test average loss: 0.03404093161225319\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 36 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2559609413146973\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.20195734500885\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7608041167259216\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1051558256149292\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.3417259454727173\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9461417198181152\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1773611307144165\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3206290006637573\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1842765808105469\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2559788227081299\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2407007217407227\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9273985624313354\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1623164415359497\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.270923137664795\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2377052307128906\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8112806081771851\n",
            "\n",
            "Total benign train accuarcy: 65.452\n",
            "Total benign train loss: 1609.4979101419449\n",
            "\n",
            "[ Test epoch: 36 ]\n",
            "\n",
            "Test accuarcy: 67.13\n",
            "Test average loss: 0.030860085490345954\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 37 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.979279637336731\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0362622737884521\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.350231647491455\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.2349239587783813\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0888346433639526\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9641828536987305\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9399475455284119\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.77052903175354\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.86307293176651\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0824958086013794\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0427473783493042\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.2526222467422485\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2125489711761475\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6274474859237671\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8217882513999939\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5828396081924438\n",
            "\n",
            "Total benign train accuarcy: 65.786\n",
            "Total benign train loss: 1597.4756170511246\n",
            "\n",
            "[ Test epoch: 37 ]\n",
            "\n",
            "Test accuarcy: 64.71\n",
            "Test average loss: 0.03317682928144932\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 38 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.5610514879226685\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9647803902626038\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1928691864013672\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9125743508338928\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8573867082595825\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9670203328132629\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9142765998840332\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6960817575454712\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.4755600690841675\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9751138687133789\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0450183153152466\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7712065577507019\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9260823726654053\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1568400859832764\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0093241930007935\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0107825994491577\n",
            "\n",
            "Total benign train accuarcy: 66.016\n",
            "Total benign train loss: 1589.1960833370686\n",
            "\n",
            "[ Test epoch: 38 ]\n",
            "\n",
            "Test accuarcy: 65.63\n",
            "Test average loss: 0.03251345421075821\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 39 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8409619927406311\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.690036416053772\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.796728253364563\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8958736062049866\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3478479385375977\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8092286586761475\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.246423602104187\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.4375\n",
            "Current benign train loss: 1.321757435798645\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1077251434326172\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9576390981674194\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8985508680343628\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9373670816421509\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.858335554599762\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8390665054321289\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1028358936309814\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8531394600868225\n",
            "\n",
            "Total benign train accuarcy: 65.98\n",
            "Total benign train loss: 1585.5059170722961\n",
            "\n",
            "[ Test epoch: 39 ]\n",
            "\n",
            "Test accuarcy: 62.57\n",
            "Test average loss: 0.03464192351102829\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 40 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1471525430679321\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1348772048950195\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2870475053787231\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.045336365699768\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.822381854057312\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7332866787910461\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.763410210609436\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1144822835922241\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0428355932235718\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9095718264579773\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0763096809387207\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9579772353172302\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4706381559371948\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.4719873666763306\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.131524682044983\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2822321653366089\n",
            "\n",
            "Total benign train accuarcy: 65.808\n",
            "Total benign train loss: 1583.9618499577045\n",
            "\n",
            "[ Test epoch: 40 ]\n",
            "\n",
            "Test accuarcy: 64.49\n",
            "Test average loss: 0.03330455996096134\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 41 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.187596082687378\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2194973230361938\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6894361972808838\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.2991827726364136\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8025583028793335\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.709833025932312\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0069762468338013\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.788545548915863\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6315777897834778\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8103492856025696\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.872318685054779\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0491496324539185\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0235984325408936\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8789415955543518\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.6100473403930664\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9890449643135071\n",
            "\n",
            "Total benign train accuarcy: 66.076\n",
            "Total benign train loss: 1576.975084811449\n",
            "\n",
            "[ Test epoch: 41 ]\n",
            "\n",
            "Test accuarcy: 67.63\n",
            "Test average loss: 0.030060636854171752\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 42 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7778521776199341\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9425925016403198\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1675913333892822\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0924084186553955\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.715368628501892\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8384690284729004\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8056378364562988\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0997920036315918\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1174830198287964\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.2865251302719116\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8829029202461243\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6500626802444458\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1426562070846558\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.530563473701477\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.9354351758956909\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.951790988445282\n",
            "\n",
            "Total benign train accuarcy: 66.12\n",
            "Total benign train loss: 1578.1716649830341\n",
            "\n",
            "[ Test epoch: 42 ]\n",
            "\n",
            "Test accuarcy: 65.55\n",
            "Test average loss: 0.0337494566231966\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 43 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9952425360679626\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0276267528533936\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2283470630645752\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9907760620117188\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8433502912521362\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6249005794525146\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0815054178237915\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1297236680984497\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7425493597984314\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3470385074615479\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2575712203979492\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.283461093902588\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6544703245162964\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3821041584014893\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.6458607912063599\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1642305850982666\n",
            "\n",
            "Total benign train accuarcy: 66.182\n",
            "Total benign train loss: 1567.5711217820644\n",
            "\n",
            "[ Test epoch: 43 ]\n",
            "\n",
            "Test accuarcy: 62.94\n",
            "Test average loss: 0.0337180385529995\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 44 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.774571418762207\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.7234289050102234\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8352536559104919\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.3258991241455078\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.2144086360931396\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8589466214179993\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6392183303833008\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.617098331451416\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1253572702407837\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.4889235496520996\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.885527491569519\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2142924070358276\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8373227715492249\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9562847018241882\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6710526943206787\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.245927095413208\n",
            "\n",
            "Total benign train accuarcy: 66.464\n",
            "Total benign train loss: 1563.2791547477245\n",
            "\n",
            "[ Test epoch: 44 ]\n",
            "\n",
            "Test accuarcy: 66.19\n",
            "Test average loss: 0.03319823858439922\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 45 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.881753146648407\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.202316164970398\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8861052393913269\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0555100440979004\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.221547245979309\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7327008843421936\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 0.858518123626709\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2057104110717773\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7901228070259094\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0945115089416504\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9443338513374329\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9413125514984131\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2212655544281006\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8690669536590576\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.423749327659607\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8226578235626221\n",
            "\n",
            "Total benign train accuarcy: 66.358\n",
            "Total benign train loss: 1565.9742512404919\n",
            "\n",
            "[ Test epoch: 45 ]\n",
            "\n",
            "Test accuarcy: 68.26\n",
            "Test average loss: 0.028914625597000122\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 46 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9591389298439026\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9801439046859741\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5288853645324707\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6592949628829956\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1929700374603271\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1409248113632202\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.040153980255127\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.01042640209198\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1761915683746338\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.158785104751587\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6948978900909424\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6872048377990723\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6343145370483398\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2153147459030151\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7643700838088989\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.012995719909668\n",
            "\n",
            "Total benign train accuarcy: 66.73\n",
            "Total benign train loss: 1553.5323504209518\n",
            "\n",
            "[ Test epoch: 46 ]\n",
            "\n",
            "Test accuarcy: 65.52\n",
            "Test average loss: 0.032586024555563926\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 47 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8638322949409485\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9154402017593384\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8703063726425171\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3707106113433838\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9135185480117798\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7511836886405945\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.9375\n",
            "Current benign train loss: 0.42599546909332275\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2308499813079834\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.3567466735839844\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.410288691520691\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5927991271018982\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0859216451644897\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9529194831848145\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9394764304161072\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0902801752090454\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.4060240983963013\n",
            "\n",
            "Total benign train accuarcy: 66.432\n",
            "Total benign train loss: 1562.835758894682\n",
            "\n",
            "[ Test epoch: 47 ]\n",
            "\n",
            "Test accuarcy: 65.98\n",
            "Test average loss: 0.032467310184240344\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 48 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7487609386444092\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7061489820480347\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.13602614402771\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1898783445358276\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1138585805892944\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9002367854118347\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.865690290927887\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8203065395355225\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0387927293777466\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7809243202209473\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.4297795295715332\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.6984437108039856\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7481417059898376\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9128972887992859\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7815098762512207\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1490648984909058\n",
            "\n",
            "Total benign train accuarcy: 66.488\n",
            "Total benign train loss: 1557.824193239212\n",
            "\n",
            "[ Test epoch: 48 ]\n",
            "\n",
            "Test accuarcy: 62.66\n",
            "Test average loss: 0.034504799628257754\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 49 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8423476815223694\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2363280057907104\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.000901699066162\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.899147093296051\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8718572854995728\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7714369893074036\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0328903198242188\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9025660157203674\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0005300045013428\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9296144247055054\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3263145685195923\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8963192105293274\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.40625\n",
            "Current benign train loss: 1.438060998916626\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9620598554611206\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.730614423751831\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1129859685897827\n",
            "\n",
            "Total benign train accuarcy: 66.926\n",
            "Total benign train loss: 1551.8560088276863\n",
            "\n",
            "[ Test epoch: 49 ]\n",
            "\n",
            "Test accuarcy: 65.67\n",
            "Test average loss: 0.03145720609724521\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 50 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9279459714889526\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.7664851546287537\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1971355676651\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.846532940864563\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0311540365219116\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5376458764076233\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1179401874542236\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0896209478378296\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.827724039554596\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9599893689155579\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.6240861415863037\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0683261156082153\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.190730333328247\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1227577924728394\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1632194519042969\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2436940670013428\n",
            "\n",
            "Total benign train accuarcy: 66.71\n",
            "Total benign train loss: 1549.8204604685307\n",
            "\n",
            "[ Test epoch: 50 ]\n",
            "\n",
            "Test accuarcy: 70.02\n",
            "Test average loss: 0.028032650512456896\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 51 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0755934715270996\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0873169898986816\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7821956872940063\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.777371883392334\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0533257722854614\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2439939975738525\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2094552516937256\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0366183519363403\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6791768670082092\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0764654874801636\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0703493356704712\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0646075010299683\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6602753400802612\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1105631589889526\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9448300004005432\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9647756814956665\n",
            "\n",
            "Total benign train accuarcy: 66.876\n",
            "Total benign train loss: 1549.8012093305588\n",
            "\n",
            "[ Test epoch: 51 ]\n",
            "\n",
            "Test accuarcy: 65.45\n",
            "Test average loss: 0.03431010613739491\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 52 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.174909234046936\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.472853422164917\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9622116088867188\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8623214960098267\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.5083087682724\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0338160991668701\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8312663435935974\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8483710289001465\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7543156743049622\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9102921485900879\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.310988187789917\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3281497955322266\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8005509376525879\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0926194190979004\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.002299189567566\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1056689023971558\n",
            "\n",
            "Total benign train accuarcy: 67.018\n",
            "Total benign train loss: 1553.0052164196968\n",
            "\n",
            "[ Test epoch: 52 ]\n",
            "\n",
            "Test accuarcy: 65.46\n",
            "Test average loss: 0.03246445278525353\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 53 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8631165623664856\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4230973720550537\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.5065491199493408\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.4693584442138672\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.4375\n",
            "Current benign train loss: 1.6164191961288452\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.803789496421814\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1182700395584106\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0757592916488647\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9832383394241333\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.8692378401756287\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3297187089920044\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7386182546615601\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.884891927242279\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9325016736984253\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7778940200805664\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.7635668516159058\n",
            "\n",
            "Total benign train accuarcy: 66.464\n",
            "Total benign train loss: 1559.839583903551\n",
            "\n",
            "[ Test epoch: 53 ]\n",
            "\n",
            "Test accuarcy: 69.03\n",
            "Test average loss: 0.029141879796981813\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 54 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6959248781204224\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1445461511611938\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2031785249710083\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6124008893966675\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1220879554748535\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.021479606628418\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2567092180252075\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2561320066452026\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9074125289916992\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0096640586853027\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.936205267906189\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8207792043685913\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6823379397392273\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9306203126907349\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0259783267974854\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8349382877349854\n",
            "\n",
            "Total benign train accuarcy: 66.684\n",
            "Total benign train loss: 1550.0210892260075\n",
            "\n",
            "[ Test epoch: 54 ]\n",
            "\n",
            "Test accuarcy: 69.24\n",
            "Test average loss: 0.0283770098477602\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 55 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0339081287384033\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8726672530174255\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0173224210739136\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7769826650619507\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9527989029884338\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.4375\n",
            "Current benign train loss: 1.8571577072143555\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5315551161766052\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9027657508850098\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1313494443893433\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0447444915771484\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.207146406173706\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0231512784957886\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9002217054367065\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0186620950698853\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0326433181762695\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.725582480430603\n",
            "\n",
            "Total benign train accuarcy: 67.132\n",
            "Total benign train loss: 1544.7657441198826\n",
            "\n",
            "[ Test epoch: 55 ]\n",
            "\n",
            "Test accuarcy: 68.4\n",
            "Test average loss: 0.029038641357421874\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 56 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8259103894233704\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6494874358177185\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9136201739311218\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0100042819976807\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6486501693725586\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.125174880027771\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2786295413970947\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0956752300262451\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8106269240379333\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7456400394439697\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9102689027786255\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.503591537475586\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0025702714920044\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3023604154586792\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2261412143707275\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.1616291999816895\n",
            "\n",
            "Total benign train accuarcy: 67.05\n",
            "Total benign train loss: 1540.1362846195698\n",
            "\n",
            "[ Test epoch: 56 ]\n",
            "\n",
            "Test accuarcy: 67.46\n",
            "Test average loss: 0.03224980570673942\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 57 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1626381874084473\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3486952781677246\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9388260841369629\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.3209298849105835\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8459612131118774\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8371421694755554\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0608566999435425\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.494039535522461\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7479724884033203\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.491426944732666\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.5641200542449951\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6609398722648621\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1786831617355347\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6591873168945312\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1067922115325928\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8352965712547302\n",
            "\n",
            "Total benign train accuarcy: 67.378\n",
            "Total benign train loss: 1539.829857468605\n",
            "\n",
            "[ Test epoch: 57 ]\n",
            "\n",
            "Test accuarcy: 70.9\n",
            "Test average loss: 0.02719541155397892\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 58 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2649937868118286\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1053122282028198\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.4375\n",
            "Current benign train loss: 1.8934574127197266\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0401360988616943\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8220879435539246\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0843795537948608\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0057651996612549\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0063472986221313\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0266293287277222\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7154977321624756\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2208648920059204\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.118349313735962\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.44649338722229\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8472303152084351\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1750621795654297\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.4375\n",
            "Current benign train loss: 1.511725902557373\n",
            "\n",
            "Total benign train accuarcy: 67.278\n",
            "Total benign train loss: 1539.1472308933735\n",
            "\n",
            "[ Test epoch: 58 ]\n",
            "\n",
            "Test accuarcy: 66.71\n",
            "Test average loss: 0.03153271745145321\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 59 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.166253924369812\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7008697986602783\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.4375\n",
            "Current benign train loss: 1.4015475511550903\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9111303091049194\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0368967056274414\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8609163761138916\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7934377193450928\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8374980688095093\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8455803394317627\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9491164684295654\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1029059886932373\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0022987127304077\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9544121026992798\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2507758140563965\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7593922019004822\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9149088263511658\n",
            "\n",
            "Total benign train accuarcy: 67.396\n",
            "Total benign train loss: 1526.1402402222157\n",
            "\n",
            "[ Test epoch: 59 ]\n",
            "\n",
            "Test accuarcy: 70.61\n",
            "Test average loss: 0.02766211660504341\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 60 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.701679527759552\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8700619339942932\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8749396800994873\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7373974919319153\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8035953640937805\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1681514978408813\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9054805040359497\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2069076299667358\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7300482988357544\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1923075914382935\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9248034358024597\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8382096886634827\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9647430777549744\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7899355292320251\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0007423162460327\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8656647205352783\n",
            "\n",
            "Total benign train accuarcy: 67.074\n",
            "Total benign train loss: 1534.4737954735756\n",
            "\n",
            "[ Test epoch: 60 ]\n",
            "\n",
            "Test accuarcy: 67.06\n",
            "Test average loss: 0.03042442302405834\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 61 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3084527254104614\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5897196531295776\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9594086408615112\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8470840454101562\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0021642446517944\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0812124013900757\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9736068844795227\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1838263273239136\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6264710426330566\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.139601469039917\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.6448403596878052\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7959744334220886\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2413978576660156\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9042396545410156\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0660858154296875\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1893320083618164\n",
            "\n",
            "Total benign train accuarcy: 67.836\n",
            "Total benign train loss: 1513.676714003086\n",
            "\n",
            "[ Test epoch: 61 ]\n",
            "\n",
            "Test accuarcy: 65.26\n",
            "Test average loss: 0.033641980892419814\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 62 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7433255314826965\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2094992399215698\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7157497406005859\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.3053604364395142\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.3292434215545654\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3519755601882935\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0011792182922363\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1309077739715576\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0359861850738525\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6779576539993286\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9047813415527344\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 1.007891297340393\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.0732570886611938\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2428745031356812\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7399597764015198\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2288095951080322\n",
            "\n",
            "Total benign train accuarcy: 67.232\n",
            "Total benign train loss: 1526.2597585618496\n",
            "\n",
            "[ Test epoch: 62 ]\n",
            "\n",
            "Test accuarcy: 63.7\n",
            "Test average loss: 0.03352864734828472\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 63 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.075711965560913\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9294540286064148\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.075394868850708\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9205607175827026\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.548527479171753\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.045284390449524\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9205021858215332\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9915711283683777\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.877946138381958\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.553447961807251\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2962502241134644\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6745097637176514\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.152312994003296\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.890995979309082\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0171325206756592\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7536341547966003\n",
            "\n",
            "Total benign train accuarcy: 67.236\n",
            "Total benign train loss: 1526.4131189584732\n",
            "\n",
            "[ Test epoch: 63 ]\n",
            "\n",
            "Test accuarcy: 66.03\n",
            "Test average loss: 0.03150974353551864\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 64 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.2401601076126099\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.008461356163025\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.5121374130249023\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0494879484176636\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6465330123901367\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.944136381149292\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1520113945007324\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6557987928390503\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0570433139801025\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7907703518867493\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2239772081375122\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8164719343185425\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7588294148445129\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0268887281417847\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7787683606147766\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.125478744506836\n",
            "\n",
            "Total benign train accuarcy: 67.136\n",
            "Total benign train loss: 1536.9119572341442\n",
            "\n",
            "[ Test epoch: 64 ]\n",
            "\n",
            "Test accuarcy: 70.58\n",
            "Test average loss: 0.027176935854554177\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 65 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3905186653137207\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8559643626213074\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7334868311882019\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7200522422790527\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8027167320251465\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6849338412284851\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7961698770523071\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0823924541473389\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7125420570373535\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9071773290634155\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1928547620773315\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8834197521209717\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.132210373878479\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7394816279411316\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0282294750213623\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1965304613113403\n",
            "\n",
            "Total benign train accuarcy: 67.546\n",
            "Total benign train loss: 1517.2097779214382\n",
            "\n",
            "[ Test epoch: 65 ]\n",
            "\n",
            "Test accuarcy: 66.34\n",
            "Test average loss: 0.03136962807178497\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 66 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2418783903121948\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8395186066627502\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8414739966392517\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7816111445426941\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.961503803730011\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.012209177017212\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1375054121017456\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.1965997219085693\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9210936427116394\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9805163741111755\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.3271088600158691\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.4320275783538818\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8633372783660889\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0425746440887451\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3033692836761475\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9561720490455627\n",
            "\n",
            "Total benign train accuarcy: 67.236\n",
            "Total benign train loss: 1524.766724795103\n",
            "\n",
            "[ Test epoch: 66 ]\n",
            "\n",
            "Test accuarcy: 71.41\n",
            "Test average loss: 0.02725316220521927\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 67 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0969140529632568\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.244887351989746\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2776093482971191\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.07163667678833\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.837981104850769\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8734314441680908\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2234796285629272\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.8795289993286133\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1201413869857788\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6436187624931335\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.162219762802124\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.903941810131073\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8979747295379639\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.020349383354187\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1120275259017944\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.038803219795227\n",
            "\n",
            "Total benign train accuarcy: 67.416\n",
            "Total benign train loss: 1526.4880122542381\n",
            "\n",
            "[ Test epoch: 67 ]\n",
            "\n",
            "Test accuarcy: 69.85\n",
            "Test average loss: 0.02852446612715721\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 68 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.711991012096405\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2308948040008545\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1703031063079834\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0191022157669067\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8527218699455261\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8116878271102905\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9470936059951782\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8676341772079468\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0434564352035522\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2833294868469238\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.4375\n",
            "Current benign train loss: 1.6997020244598389\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.6568138003349304\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8309000730514526\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7367406487464905\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9943172931671143\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 0.967107892036438\n",
            "\n",
            "Total benign train accuarcy: 67.498\n",
            "Total benign train loss: 1520.005451709032\n",
            "\n",
            "[ Test epoch: 68 ]\n",
            "\n",
            "Test accuarcy: 67.85\n",
            "Test average loss: 0.03038359473645687\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 69 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9971975684165955\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9934711456298828\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0976072549819946\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8938645720481873\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8445897102355957\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7790789008140564\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8693994283676147\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3139311075210571\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9634996056556702\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0153746604919434\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8268511295318604\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.593151330947876\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9927334785461426\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1454354524612427\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.036872386932373\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3353062868118286\n",
            "\n",
            "Total benign train accuarcy: 67.372\n",
            "Total benign train loss: 1526.8491164147854\n",
            "\n",
            "[ Test epoch: 69 ]\n",
            "\n",
            "Test accuarcy: 65.72\n",
            "Test average loss: 0.031544477650523185\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 70 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.164466381072998\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9251272678375244\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.081097960472107\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.4703575372695923\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2337192296981812\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0752627849578857\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7182175517082214\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6533217430114746\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.12712562084198\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6732851266860962\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1597257852554321\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8056670427322388\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8567237257957458\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9563036561012268\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0593464374542236\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7927063703536987\n",
            "\n",
            "Total benign train accuarcy: 67.954\n",
            "Total benign train loss: 1506.6713153421879\n",
            "\n",
            "[ Test epoch: 70 ]\n",
            "\n",
            "Test accuarcy: 70.66\n",
            "Test average loss: 0.028237986403703688\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 71 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9165656566619873\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1316344738006592\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9672107100486755\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6035732626914978\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.5572389960289001\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.8364108800888062\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2416049242019653\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.5238747596740723\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8894789218902588\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.370539665222168\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0014681816101074\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3931952714920044\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2103592157363892\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8859667778015137\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6061429977416992\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2783544063568115\n",
            "\n",
            "Total benign train accuarcy: 67.57\n",
            "Total benign train loss: 1516.5846030414104\n",
            "\n",
            "[ Test epoch: 71 ]\n",
            "\n",
            "Test accuarcy: 61.68\n",
            "Test average loss: 0.038216448199748995\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 72 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8062636256217957\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2366728782653809\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.009867787361145\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8998572826385498\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1446990966796875\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9468323588371277\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9559165239334106\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9876639246940613\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0314950942993164\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0299501419067383\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0929793119430542\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8326789736747742\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.107407569885254\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8453767895698547\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3053197860717773\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.121626615524292\n",
            "\n",
            "Total benign train accuarcy: 67.506\n",
            "Total benign train loss: 1518.0129162371159\n",
            "\n",
            "[ Test epoch: 72 ]\n",
            "\n",
            "Test accuarcy: 68.52\n",
            "Test average loss: 0.02954298960864544\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 73 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2166634798049927\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8646556735038757\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8771041035652161\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5987525582313538\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2329779863357544\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8806255459785461\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.96355801820755\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7909896969795227\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.754356861114502\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9399673342704773\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1517040729522705\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8228988647460938\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9753669500350952\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9922478199005127\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8818594813346863\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9729802012443542\n",
            "\n",
            "Total benign train accuarcy: 67.7\n",
            "Total benign train loss: 1520.4949017167091\n",
            "\n",
            "[ Test epoch: 73 ]\n",
            "\n",
            "Test accuarcy: 67.5\n",
            "Test average loss: 0.02929268981218338\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 74 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9852049350738525\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9452722072601318\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1781355142593384\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0041414499282837\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.948050856590271\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7186843156814575\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0667246580123901\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9742687940597534\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.110890507698059\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7386646866798401\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.229025959968567\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8604394793510437\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.5319888591766357\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9505376815795898\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9574161767959595\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6439594626426697\n",
            "\n",
            "Total benign train accuarcy: 67.586\n",
            "Total benign train loss: 1519.7491835951805\n",
            "\n",
            "[ Test epoch: 74 ]\n",
            "\n",
            "Test accuarcy: 66.65\n",
            "Test average loss: 0.030880833759903907\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 75 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3720440864562988\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.319832444190979\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7399045825004578\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9075844287872314\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8575195074081421\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7933101654052734\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7530938386917114\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7419165968894958\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9949926733970642\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.1160260438919067\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8604849576950073\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.354707956314087\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7717597484588623\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.124017596244812\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7926334738731384\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.7919514179229736\n",
            "\n",
            "Total benign train accuarcy: 67.88\n",
            "Total benign train loss: 1507.1945250034332\n",
            "\n",
            "[ Test epoch: 75 ]\n",
            "\n",
            "Test accuarcy: 67.2\n",
            "Test average loss: 0.030683950328826905\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 76 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8089767098426819\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7009274959564209\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8833867311477661\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7934780120849609\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7347495555877686\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1302303075790405\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6903597116470337\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.71232670545578\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2092435359954834\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.163193702697754\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0063645839691162\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.140628457069397\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.13910973072052\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.055934190750122\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9204426407814026\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7745288014411926\n",
            "\n",
            "Total benign train accuarcy: 67.716\n",
            "Total benign train loss: 1517.6393238604069\n",
            "\n",
            "[ Test epoch: 76 ]\n",
            "\n",
            "Test accuarcy: 65.28\n",
            "Test average loss: 0.03374738872349262\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 77 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8159456253051758\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7749152779579163\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.880932092666626\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8225290775299072\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6957688927650452\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2848185300827026\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8714972138404846\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7736656665802002\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9449184536933899\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1379824876785278\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.002878189086914\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2635523080825806\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.136833667755127\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.5195073485374451\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.3269058465957642\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.8364983797073364\n",
            "\n",
            "Total benign train accuarcy: 67.876\n",
            "Total benign train loss: 1504.7167829871178\n",
            "\n",
            "[ Test epoch: 77 ]\n",
            "\n",
            "Test accuarcy: 71.06\n",
            "Test average loss: 0.026751011061668398\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 78 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9474310278892517\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6254926323890686\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5870997905731201\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8611906170845032\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7222782969474792\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7724446058273315\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9677783250808716\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.510010004043579\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1430190801620483\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7363878488540649\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0762557983398438\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.8567754030227661\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.115265965461731\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5811228156089783\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0109899044036865\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0912572145462036\n",
            "\n",
            "Total benign train accuarcy: 67.692\n",
            "Total benign train loss: 1511.754813104868\n",
            "\n",
            "[ Test epoch: 78 ]\n",
            "\n",
            "Test accuarcy: 67.55\n",
            "Test average loss: 0.03127736376225948\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 79 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.5774613618850708\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.6049935817718506\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3234728574752808\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.121299147605896\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.7427613735198975\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1732836961746216\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0432453155517578\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.00189208984375\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9821796417236328\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9354526400566101\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.979272723197937\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1909205913543701\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0754802227020264\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8802717328071594\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7523219585418701\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2162244319915771\n",
            "\n",
            "Total benign train accuarcy: 67.472\n",
            "Total benign train loss: 1515.1620175540447\n",
            "\n",
            "[ Test epoch: 79 ]\n",
            "\n",
            "Test accuarcy: 67.34\n",
            "Test average loss: 0.030354930931329727\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 80 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9814466238021851\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7843921184539795\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8941651582717896\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3243204355239868\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7018612623214722\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8486488461494446\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7750336527824402\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9845873117446899\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9686017632484436\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.132965087890625\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7761332988739014\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.030982494354248\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.991405725479126\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0785984992980957\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0699148178100586\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7406995296478271\n",
            "\n",
            "Total benign train accuarcy: 68.106\n",
            "Total benign train loss: 1494.9666979312897\n",
            "\n",
            "[ Test epoch: 80 ]\n",
            "\n",
            "Test accuarcy: 69.9\n",
            "Test average loss: 0.02812675983607769\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 81 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8517077565193176\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2076236009597778\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0291622877120972\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8260431885719299\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.5527933239936829\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8151698708534241\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.0925734043121338\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0819897651672363\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0904028415679932\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0270329713821411\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9504684209823608\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9727300405502319\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6671267151832581\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7318680286407471\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9939128160476685\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9753903150558472\n",
            "\n",
            "Total benign train accuarcy: 67.852\n",
            "Total benign train loss: 1507.1628212034702\n",
            "\n",
            "[ Test epoch: 81 ]\n",
            "\n",
            "Test accuarcy: 64.85\n",
            "Test average loss: 0.033343295580148695\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 82 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.105811595916748\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8966016173362732\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.226102352142334\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1477657556533813\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6408483982086182\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9471997618675232\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.7006332874298096\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1692739725112915\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.823652982711792\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9287534952163696\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.074076533317566\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.12427818775177\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.960111677646637\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8593311309814453\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8682339787483215\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7048741579055786\n",
            "\n",
            "Total benign train accuarcy: 67.68\n",
            "Total benign train loss: 1510.9297235310078\n",
            "\n",
            "[ Test epoch: 82 ]\n",
            "\n",
            "Test accuarcy: 66.16\n",
            "Test average loss: 0.03145216218829155\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 83 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8755139112472534\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.237660527229309\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8164538741111755\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9629513621330261\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8063828349113464\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.7632157206535339\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6040605306625366\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9624606370925903\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8866528272628784\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0451133251190186\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7501506209373474\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9348704814910889\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9460722804069519\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8994174003601074\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2690001726150513\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0994144678115845\n",
            "\n",
            "Total benign train accuarcy: 67.386\n",
            "Total benign train loss: 1520.923884242773\n",
            "\n",
            "[ Test epoch: 83 ]\n",
            "\n",
            "Test accuarcy: 70.49\n",
            "Test average loss: 0.02767375900745392\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 84 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8551217913627625\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7664054036140442\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.079332947731018\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7068253755569458\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7677903771400452\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.657263457775116\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9959442019462585\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9741214513778687\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9876858592033386\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.229560136795044\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7799817323684692\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6665066480636597\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0101573467254639\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7804240584373474\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1418355703353882\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1491960287094116\n",
            "\n",
            "Total benign train accuarcy: 68.046\n",
            "Total benign train loss: 1496.980733513832\n",
            "\n",
            "[ Test epoch: 84 ]\n",
            "\n",
            "Test accuarcy: 69.85\n",
            "Test average loss: 0.028094852298498153\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 85 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0655936002731323\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.637507975101471\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8827824592590332\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3842391967773438\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2376799583435059\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6676564812660217\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6315227150917053\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7931775450706482\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8873712420463562\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7662774324417114\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6097255945205688\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6865251064300537\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1052154302597046\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.4042861461639404\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1683809757232666\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.4560527801513672\n",
            "\n",
            "Total benign train accuarcy: 67.866\n",
            "Total benign train loss: 1507.6168020367622\n",
            "\n",
            "[ Test epoch: 85 ]\n",
            "\n",
            "Test accuarcy: 66.34\n",
            "Test average loss: 0.03184727631211281\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 86 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0396236181259155\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0612056255340576\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.0943320989608765\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0034167766571045\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9231858253479004\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6989122629165649\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8802023530006409\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1771025657653809\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8836194276809692\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1910145282745361\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6393010020256042\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9744282960891724\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3550865650177002\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3743380308151245\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0022486448287964\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.787872314453125\n",
            "\n",
            "Total benign train accuarcy: 67.814\n",
            "Total benign train loss: 1508.0853742957115\n",
            "\n",
            "[ Test epoch: 86 ]\n",
            "\n",
            "Test accuarcy: 66.12\n",
            "Test average loss: 0.03214354086816311\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 87 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9574984312057495\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2605167627334595\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2034227848052979\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.011584758758545\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2072638273239136\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2734520435333252\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.9344180226325989\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7935131788253784\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.545175313949585\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6964781880378723\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9242419004440308\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8103702068328857\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1334377527236938\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2412841320037842\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9069364666938782\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8918084502220154\n",
            "\n",
            "Total benign train accuarcy: 67.772\n",
            "Total benign train loss: 1501.2322271168232\n",
            "\n",
            "[ Test epoch: 87 ]\n",
            "\n",
            "Test accuarcy: 69.06\n",
            "Test average loss: 0.028292360043525696\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 88 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.5689915418624878\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7589202523231506\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7964457869529724\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1443127393722534\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0214282274246216\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1475886106491089\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.7435377836227417\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8769780397415161\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.191650390625\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8771004676818848\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9831443428993225\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.852271556854248\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8268495202064514\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8980665802955627\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0767860412597656\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1889199018478394\n",
            "\n",
            "Total benign train accuarcy: 68.216\n",
            "Total benign train loss: 1486.1795448958874\n",
            "\n",
            "[ Test epoch: 88 ]\n",
            "\n",
            "Test accuarcy: 70.97\n",
            "Test average loss: 0.027364491257071494\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 89 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.9065784215927124\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7921586036682129\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.623778760433197\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.6493581533432007\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2253059148788452\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.4732471704483032\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8525806665420532\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1978754997253418\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3708341121673584\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.22564697265625\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0654237270355225\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7641982436180115\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.6944650411605835\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.365815281867981\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9286819696426392\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.3664673566818237\n",
            "\n",
            "Total benign train accuarcy: 67.99\n",
            "Total benign train loss: 1504.9417878091335\n",
            "\n",
            "[ Test epoch: 89 ]\n",
            "\n",
            "Test accuarcy: 69.38\n",
            "Test average loss: 0.028852078053355216\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 90 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1301809549331665\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0407326221466064\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8359892964363098\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9666131734848022\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7795593738555908\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2317132949829102\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8332080841064453\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.1151620149612427\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8356596231460571\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.281779408454895\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9690152406692505\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9074618816375732\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1687983274459839\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.169190526008606\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9809507131576538\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9693537950515747\n",
            "\n",
            "Total benign train accuarcy: 67.82\n",
            "Total benign train loss: 1505.5507752895355\n",
            "\n",
            "[ Test epoch: 90 ]\n",
            "\n",
            "Test accuarcy: 63.67\n",
            "Test average loss: 0.03737898095846176\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 91 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7662993669509888\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.010632872581482\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1460163593292236\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0448857545852661\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9694629311561584\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7349665760993958\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.5500949025154114\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0917223691940308\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9105904698371887\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9127782583236694\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9409087300300598\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9834926128387451\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8443146347999573\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8867117166519165\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9744617342948914\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1594810485839844\n",
            "\n",
            "Total benign train accuarcy: 67.99\n",
            "Total benign train loss: 1500.178326755762\n",
            "\n",
            "[ Test epoch: 91 ]\n",
            "\n",
            "Test accuarcy: 67.03\n",
            "Test average loss: 0.030572073808312415\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 92 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0523202419281006\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.3584128618240356\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.4921088218688965\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6603086590766907\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9479055404663086\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9307330250740051\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9800921082496643\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9132383465766907\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7068006992340088\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.0189827680587769\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9266669154167175\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9540773034095764\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.5611008405685425\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8764175176620483\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.961211621761322\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.865817129611969\n",
            "\n",
            "Total benign train accuarcy: 67.916\n",
            "Total benign train loss: 1503.2786708176136\n",
            "\n",
            "[ Test epoch: 92 ]\n",
            "\n",
            "Test accuarcy: 63.95\n",
            "Test average loss: 0.03485390647053718\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 93 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0710407495498657\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.8269196152687073\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0805531740188599\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.3888170719146729\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.4222633838653564\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9705988168716431\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9088326692581177\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.929822564125061\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 0.9852457046508789\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0187832117080688\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.839797854423523\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9498503804206848\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0986158847808838\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.114562749862671\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1126261949539185\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0747849941253662\n",
            "\n",
            "Total benign train accuarcy: 68.032\n",
            "Total benign train loss: 1493.2735524475574\n",
            "\n",
            "[ Test epoch: 93 ]\n",
            "\n",
            "Test accuarcy: 65.97\n",
            "Test average loss: 0.032612622278928756\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 94 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.5960966348648071\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.222578525543213\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.099364995956421\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9826836585998535\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9401843547821045\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9454691410064697\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9234050512313843\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0390163660049438\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8557980060577393\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9437358379364014\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6820418238639832\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7713555693626404\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9903312921524048\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8335990309715271\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.9634467363357544\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.1434237957000732\n",
            "\n",
            "Total benign train accuarcy: 68.088\n",
            "Total benign train loss: 1505.5164360702038\n",
            "\n",
            "[ Test epoch: 94 ]\n",
            "\n",
            "Test accuarcy: 70.5\n",
            "Test average loss: 0.027504152420163155\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 95 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1338545083999634\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7015395164489746\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.7769392132759094\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0041972398757935\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.4347494840621948\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3365458250045776\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.806792676448822\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.162292242050171\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.4077214002609253\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.0329076051712036\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8745487928390503\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.6355686187744141\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.2058004140853882\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2629235982894897\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.4995034337043762\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9920129179954529\n",
            "\n",
            "Total benign train accuarcy: 68.038\n",
            "Total benign train loss: 1495.0388547480106\n",
            "\n",
            "[ Test epoch: 95 ]\n",
            "\n",
            "Test accuarcy: 66.94\n",
            "Test average loss: 0.030891827568411828\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 96 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.4208870232105255\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.692714273929596\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.2518974542617798\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9138975143432617\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.799034059047699\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2183434963226318\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7628604769706726\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.6780062317848206\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.640992283821106\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.883610188961029\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.4267011880874634\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2171630859375\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.0129692554473877\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.8393669724464417\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.4317265748977661\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.9469265341758728\n",
            "\n",
            "Total benign train accuarcy: 68.248\n",
            "Total benign train loss: 1493.8038646876812\n",
            "\n",
            "[ Test epoch: 96 ]\n",
            "\n",
            "Test accuarcy: 69.03\n",
            "Test average loss: 0.029525953447818755\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 97 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.3497227430343628\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.119461178779602\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.568186342716217\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.119770884513855\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.1361316442489624\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.3318805694580078\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 0.985113799571991\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.6196473836898804\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8808237910270691\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8866466283798218\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 1.0012167692184448\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8621939420700073\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.1775267124176025\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 0.960670530796051\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.1993322372436523\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.836821973323822\n",
            "\n",
            "Total benign train accuarcy: 67.668\n",
            "Total benign train loss: 1517.2785734832287\n",
            "\n",
            "[ Test epoch: 97 ]\n",
            "\n",
            "Test accuarcy: 71.25\n",
            "Test average loss: 0.026742019188404083\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 98 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8701955080032349\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.0809483528137207\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8504296541213989\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.739486038684845\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8769131898880005\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2833974361419678\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7796838879585266\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9606377482414246\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.1561473608016968\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.211456060409546\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7800773978233337\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 1.0791761875152588\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.8656364679336548\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.166911244392395\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.947214663028717\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.8159990310668945\n",
            "\n",
            "Total benign train accuarcy: 67.974\n",
            "Total benign train loss: 1504.517384558916\n",
            "\n",
            "[ Test epoch: 98 ]\n",
            "\n",
            "Test accuarcy: 70.52\n",
            "Test average loss: 0.027692780035734176\n",
            "Model Saved!\n",
            "\n",
            "[ Train epoch: 99 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9044390320777893\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5776089429855347\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.9367035627365112\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8224720358848572\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.78125\n",
            "Current benign train loss: 0.7818651795387268\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.7533003091812134\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.539418637752533\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9303826689720154\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2654634714126587\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9316160678863525\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 0.8604350090026855\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.65625\n",
            "Current benign train loss: 0.9482946395874023\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.2920289039611816\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.330261468887329\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.71875\n",
            "Current benign train loss: 0.8293880224227905\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 1.024823546409607\n",
            "\n",
            "Total benign train accuarcy: 68.374\n",
            "Total benign train loss: 1488.1372882425785\n",
            "\n",
            "[ Test epoch: 99 ]\n",
            "\n",
            "Test accuarcy: 65.63\n",
            "Test average loss: 0.03229624840915203\n",
            "Model Saved!\n"
          ]
        }
      ],
      "source": [
        "# for epoch in range(0, 60):\n",
        "log_train_total_accuracy =[]\n",
        "log_train_total_loss = []\n",
        "log_test_total_accuracy =[]\n",
        "log_test_total_loss =[]\n",
        "\n",
        "\n",
        "for epoch in range(0, 100):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    train_total_accuracy, train_total_loss = train(epoch)\n",
        "    test_total_accuracy, test_total_loss = test(epoch)\n",
        "    log_train_total_accuracy.append(train_total_accuracy)\n",
        "    log_train_total_loss.append(train_total_loss)\n",
        "    log_test_total_accuracy.append(test_total_accuracy)\n",
        "    log_test_total_loss.append(test_total_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLQzz8nykn2R"
      },
      "source": [
        "5. Plotting Train Accuracy & Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQd8X8c8iGw8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f7219dae-ae32-444e-effe-5537ea01d6ba"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "epoch =[]\n",
        "\n",
        "for i , loss in enumerate(log_test_total_loss):\n",
        "  epoch.append(i+1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('log_train_accuracy')\n",
        "plt.plot(epoch , log_train_total_accuracy)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('train_accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oxOu_FeiMyB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6b5bb39d-b05f-434f-ade9-dfabf6db154c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('log_train_loss')\n",
        "plt.plot(epoch , log_train_total_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('train_loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-D79-mzkuML"
      },
      "source": [
        "6. Plotting Test Accuracy & Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vygvxsl6iOpl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f5ebb782-d47f-42d5-d264-1b44896cfaea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('log_test_accuracy')\n",
        "plt.plot(epoch , log_test_total_accuracy)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('test_accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8h-hfekiRDo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3cd453d6-1643-422f-dab3-110a8fef6dbb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('log_test_loss')\n",
        "plt.plot(epoch , log_test_total_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('test_loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkGVx8krkx4_"
      },
      "source": [
        "7. Print Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXoGTukRiTpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531fd65e-3697-435c-d998-f1a7d6c4692f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15.838, 19.134, 23.052, 27.774, 32.042, 35.234, 38.256, 39.522, 44.11, 47.032, 50.428, 53.17, 55.016, 56.04, 57.716, 58.456, 59.314, 60.156, 60.492, 61.12, 61.804, 61.896, 62.22, 62.794, 63.316, 63.508, 63.712, 64.388, 64.2, 64.644, 64.706, 65.362, 65.322, 65.414, 65.318, 65.694, 65.452, 65.786, 66.016, 65.98, 65.808, 66.076, 66.12, 66.182, 66.464, 66.358, 66.73, 66.432, 66.488, 66.926, 66.71, 66.876, 67.018, 66.464, 66.684, 67.132, 67.05, 67.378, 67.278, 67.396, 67.074, 67.836, 67.232, 67.236, 67.136, 67.546, 67.236, 67.416, 67.498, 67.372, 67.954, 67.57, 67.506, 67.7, 67.586, 67.88, 67.716, 67.876, 67.692, 67.472, 68.106, 67.852, 67.68, 67.386, 68.046, 67.866, 67.814, 67.772, 68.216, 67.99, 67.82, 67.99, 67.916, 68.032, 68.088, 68.038, 68.248, 67.668, 67.974, 68.374]\n",
            "[13185.510560393333, 3341.545374393463, 3194.419829249382, 2976.9513354301453, 2823.5993859767914, 2699.99276971817, 2620.920048236847, 2569.1613911390305, 2408.911397755146, 2309.741165995598, 2187.58335852623, 2084.516088426113, 2003.2034680843353, 1967.951711177826, 1909.4486752152443, 1876.4906297922134, 1848.5090435743332, 1815.0618655085564, 1798.3837521672249, 1779.3794708251953, 1750.5306417346, 1739.5362337827682, 1729.667956829071, 1712.06420981884, 1689.8378812074661, 1679.3979397416115, 1667.7596147060394, 1643.8091043233871, 1661.6462619304657, 1642.7882291674614, 1631.521999835968, 1612.3541651368141, 1608.7131246626377, 1609.9155994057655, 1608.6464525461197, 1593.701498657465, 1609.4979101419449, 1597.4756170511246, 1589.1960833370686, 1585.5059170722961, 1583.9618499577045, 1576.975084811449, 1578.1716649830341, 1567.5711217820644, 1563.2791547477245, 1565.9742512404919, 1553.5323504209518, 1562.835758894682, 1557.824193239212, 1551.8560088276863, 1549.8204604685307, 1549.8012093305588, 1553.0052164196968, 1559.839583903551, 1550.0210892260075, 1544.7657441198826, 1540.1362846195698, 1539.829857468605, 1539.1472308933735, 1526.1402402222157, 1534.4737954735756, 1513.676714003086, 1526.2597585618496, 1526.4131189584732, 1536.9119572341442, 1517.2097779214382, 1524.766724795103, 1526.4880122542381, 1520.005451709032, 1526.8491164147854, 1506.6713153421879, 1516.5846030414104, 1518.0129162371159, 1520.4949017167091, 1519.7491835951805, 1507.1945250034332, 1517.6393238604069, 1504.7167829871178, 1511.754813104868, 1515.1620175540447, 1494.9666979312897, 1507.1628212034702, 1510.9297235310078, 1520.923884242773, 1496.980733513832, 1507.6168020367622, 1508.0853742957115, 1501.2322271168232, 1486.1795448958874, 1504.9417878091335, 1505.5507752895355, 1500.178326755762, 1503.2786708176136, 1493.2735524475574, 1505.5164360702038, 1495.0388547480106, 1493.8038646876812, 1517.2785734832287, 1504.517384558916, 1488.1372882425785]\n",
            "[19.35, 19.75, 24.92, 30.19, 32.8, 40.65, 42.53, 46.69, 48.98, 51.61, 53.52, 53.37, 59.14, 56.28, 59.2, 55.57, 64.35, 62.57, 60.02, 57.8, 61.5, 64.35, 65.08, 65.8, 60.52, 68.88, 66.96, 64.25, 65.7, 67.27, 66.01, 64.06, 59.19, 62.64, 65.08, 64.8, 67.13, 64.71, 65.63, 62.57, 64.49, 67.63, 65.55, 62.94, 66.19, 68.26, 65.52, 65.98, 62.66, 65.67, 70.02, 65.45, 65.46, 69.03, 69.24, 68.4, 67.46, 70.9, 66.71, 70.61, 67.06, 65.26, 63.7, 66.03, 70.58, 66.34, 71.41, 69.85, 67.85, 65.72, 70.66, 61.68, 68.52, 67.5, 66.65, 67.2, 65.28, 71.06, 67.55, 67.34, 69.9, 64.85, 66.16, 70.49, 69.85, 66.34, 66.12, 69.06, 70.97, 69.38, 63.67, 67.03, 63.95, 65.97, 70.5, 66.94, 69.03, 71.25, 70.52, 65.63]\n",
            "[0.09769028409719467, 0.10917206419706345, 0.08401525593996048, 0.07866197030544281, 0.11972137175798415, 0.05326175321340561, 0.05437809714078903, 0.04600277170538902, 0.04483904404044151, 0.04185743020176887, 0.04164220548272133, 0.04341315081119537, 0.03692319777011872, 0.04105006273984909, 0.03729994469285011, 0.038349568194150925, 0.03300745745897293, 0.035569740480184554, 0.03864599251151085, 0.038339744937419894, 0.034865945315361024, 0.03249189906716347, 0.031523202177882195, 0.030890709066390992, 0.035906342971324924, 0.02815065312087536, 0.031711392626166346, 0.03370989043116569, 0.03283537828922272, 0.029854828402400016, 0.031144473519921303, 0.033289601349830625, 0.03901087409853935, 0.03362874470949173, 0.03294923908114433, 0.03404093161225319, 0.030860085490345954, 0.03317682928144932, 0.03251345421075821, 0.03464192351102829, 0.03330455996096134, 0.030060636854171752, 0.0337494566231966, 0.0337180385529995, 0.03319823858439922, 0.028914625597000122, 0.032586024555563926, 0.032467310184240344, 0.034504799628257754, 0.03145720609724521, 0.028032650512456896, 0.03431010613739491, 0.03246445278525353, 0.029141879796981813, 0.0283770098477602, 0.029038641357421874, 0.03224980570673942, 0.02719541155397892, 0.03153271745145321, 0.02766211660504341, 0.03042442302405834, 0.033641980892419814, 0.03352864734828472, 0.03150974353551864, 0.027176935854554177, 0.03136962807178497, 0.02725316220521927, 0.02852446612715721, 0.03038359473645687, 0.031544477650523185, 0.028237986403703688, 0.038216448199748995, 0.02954298960864544, 0.02929268981218338, 0.030880833759903907, 0.030683950328826905, 0.03374738872349262, 0.026751011061668398, 0.03127736376225948, 0.030354930931329727, 0.02812675983607769, 0.033343295580148695, 0.03145216218829155, 0.02767375900745392, 0.028094852298498153, 0.03184727631211281, 0.03214354086816311, 0.028292360043525696, 0.027364491257071494, 0.028852078053355216, 0.03737898095846176, 0.030572073808312415, 0.03485390647053718, 0.032612622278928756, 0.027504152420163155, 0.030891827568411828, 0.029525953447818755, 0.026742019188404083, 0.027692780035734176, 0.03229624840915203]\n"
          ]
        }
      ],
      "source": [
        "print(log_train_total_accuracy)\n",
        "print(log_train_total_loss) \n",
        "print(log_test_total_accuracy)\n",
        "print(log_test_total_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXxT_BbXk1ky"
      },
      "source": [
        "8. Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMm3jA2WiWHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd72b71e-233c-420d-a86a-f500b10e444d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5            [-1, 256, 8, 8]          16,384\n",
            "       BatchNorm2d-6            [-1, 256, 8, 8]             512\n",
            "        Conv2dAuto-7             [-1, 64, 8, 8]           4,096\n",
            "       BatchNorm2d-8             [-1, 64, 8, 8]             128\n",
            "              ReLU-9             [-1, 64, 8, 8]               0\n",
            "       Conv2dAuto-10             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-11             [-1, 64, 8, 8]             128\n",
            "             ReLU-12             [-1, 64, 8, 8]               0\n",
            "       Conv2dAuto-13            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
            "ResNetBottleNeckBlock-15            [-1, 256, 8, 8]               0\n",
            "       Conv2dAuto-16             [-1, 64, 8, 8]          16,384\n",
            "      BatchNorm2d-17             [-1, 64, 8, 8]             128\n",
            "             ReLU-18             [-1, 64, 8, 8]               0\n",
            "       Conv2dAuto-19             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-20             [-1, 64, 8, 8]             128\n",
            "             ReLU-21             [-1, 64, 8, 8]               0\n",
            "       Conv2dAuto-22            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-23            [-1, 256, 8, 8]             512\n",
            "ResNetBottleNeckBlock-24            [-1, 256, 8, 8]               0\n",
            "       Conv2dAuto-25             [-1, 64, 8, 8]          16,384\n",
            "      BatchNorm2d-26             [-1, 64, 8, 8]             128\n",
            "             ReLU-27             [-1, 64, 8, 8]               0\n",
            "       Conv2dAuto-28             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-29             [-1, 64, 8, 8]             128\n",
            "             ReLU-30             [-1, 64, 8, 8]               0\n",
            "       Conv2dAuto-31            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-32            [-1, 256, 8, 8]             512\n",
            "ResNetBottleNeckBlock-33            [-1, 256, 8, 8]               0\n",
            "      ResNetLayer-34            [-1, 256, 8, 8]               0\n",
            "           Conv2d-35            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-36            [-1, 512, 4, 4]           1,024\n",
            "       Conv2dAuto-37            [-1, 128, 8, 8]          32,768\n",
            "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
            "             ReLU-39            [-1, 128, 8, 8]               0\n",
            "       Conv2dAuto-40            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
            "             ReLU-42            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-43            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
            "ResNetBottleNeckBlock-45            [-1, 512, 4, 4]               0\n",
            "       Conv2dAuto-46            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-47            [-1, 128, 4, 4]             256\n",
            "             ReLU-48            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-49            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
            "             ReLU-51            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-52            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-53            [-1, 512, 4, 4]           1,024\n",
            "ResNetBottleNeckBlock-54            [-1, 512, 4, 4]               0\n",
            "       Conv2dAuto-55            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-56            [-1, 128, 4, 4]             256\n",
            "             ReLU-57            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-58            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-59            [-1, 128, 4, 4]             256\n",
            "             ReLU-60            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-61            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-62            [-1, 512, 4, 4]           1,024\n",
            "ResNetBottleNeckBlock-63            [-1, 512, 4, 4]               0\n",
            "       Conv2dAuto-64            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-65            [-1, 128, 4, 4]             256\n",
            "             ReLU-66            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-67            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-68            [-1, 128, 4, 4]             256\n",
            "             ReLU-69            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-70            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-71            [-1, 512, 4, 4]           1,024\n",
            "ResNetBottleNeckBlock-72            [-1, 512, 4, 4]               0\n",
            "       Conv2dAuto-73            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-74            [-1, 128, 4, 4]             256\n",
            "             ReLU-75            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-76            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-77            [-1, 128, 4, 4]             256\n",
            "             ReLU-78            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-79            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-80            [-1, 512, 4, 4]           1,024\n",
            "ResNetBottleNeckBlock-81            [-1, 512, 4, 4]               0\n",
            "       Conv2dAuto-82            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-83            [-1, 128, 4, 4]             256\n",
            "             ReLU-84            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-85            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-86            [-1, 128, 4, 4]             256\n",
            "             ReLU-87            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-88            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-89            [-1, 512, 4, 4]           1,024\n",
            "ResNetBottleNeckBlock-90            [-1, 512, 4, 4]               0\n",
            "       Conv2dAuto-91            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-92            [-1, 128, 4, 4]             256\n",
            "             ReLU-93            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-94            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-95            [-1, 128, 4, 4]             256\n",
            "             ReLU-96            [-1, 128, 4, 4]               0\n",
            "       Conv2dAuto-97            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-98            [-1, 512, 4, 4]           1,024\n",
            "ResNetBottleNeckBlock-99            [-1, 512, 4, 4]               0\n",
            "      Conv2dAuto-100            [-1, 128, 4, 4]          65,536\n",
            "     BatchNorm2d-101            [-1, 128, 4, 4]             256\n",
            "            ReLU-102            [-1, 128, 4, 4]               0\n",
            "      Conv2dAuto-103            [-1, 128, 4, 4]         147,456\n",
            "     BatchNorm2d-104            [-1, 128, 4, 4]             256\n",
            "            ReLU-105            [-1, 128, 4, 4]               0\n",
            "      Conv2dAuto-106            [-1, 512, 4, 4]          65,536\n",
            "     BatchNorm2d-107            [-1, 512, 4, 4]           1,024\n",
            "ResNetBottleNeckBlock-108            [-1, 512, 4, 4]               0\n",
            "     ResNetLayer-109            [-1, 512, 4, 4]               0\n",
            "          Conv2d-110           [-1, 1024, 2, 2]         524,288\n",
            "     BatchNorm2d-111           [-1, 1024, 2, 2]           2,048\n",
            "      Conv2dAuto-112            [-1, 256, 4, 4]         131,072\n",
            "     BatchNorm2d-113            [-1, 256, 4, 4]             512\n",
            "            ReLU-114            [-1, 256, 4, 4]               0\n",
            "      Conv2dAuto-115            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-116            [-1, 256, 2, 2]             512\n",
            "            ReLU-117            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-118           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-119           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-120           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-121            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-122            [-1, 256, 2, 2]             512\n",
            "            ReLU-123            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-124            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-125            [-1, 256, 2, 2]             512\n",
            "            ReLU-126            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-127           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-129           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-130            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-131            [-1, 256, 2, 2]             512\n",
            "            ReLU-132            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-133            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-134            [-1, 256, 2, 2]             512\n",
            "            ReLU-135            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-136           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-137           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-138           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-139            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-140            [-1, 256, 2, 2]             512\n",
            "            ReLU-141            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-142            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-143            [-1, 256, 2, 2]             512\n",
            "            ReLU-144            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-145           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-146           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-147           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-148            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-149            [-1, 256, 2, 2]             512\n",
            "            ReLU-150            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-151            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-152            [-1, 256, 2, 2]             512\n",
            "            ReLU-153            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-154           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-155           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-156           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-157            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-158            [-1, 256, 2, 2]             512\n",
            "            ReLU-159            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-160            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-161            [-1, 256, 2, 2]             512\n",
            "            ReLU-162            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-163           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-164           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-165           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-166            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-167            [-1, 256, 2, 2]             512\n",
            "            ReLU-168            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-169            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-170            [-1, 256, 2, 2]             512\n",
            "            ReLU-171            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-172           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-173           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-174           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-175            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-176            [-1, 256, 2, 2]             512\n",
            "            ReLU-177            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-178            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-179            [-1, 256, 2, 2]             512\n",
            "            ReLU-180            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-181           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-182           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-183           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-184            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-185            [-1, 256, 2, 2]             512\n",
            "            ReLU-186            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-187            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-188            [-1, 256, 2, 2]             512\n",
            "            ReLU-189            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-190           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-191           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-192           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-193            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-194            [-1, 256, 2, 2]             512\n",
            "            ReLU-195            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-196            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-197            [-1, 256, 2, 2]             512\n",
            "            ReLU-198            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-199           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-200           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-201           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-202            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-203            [-1, 256, 2, 2]             512\n",
            "            ReLU-204            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-205            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-206            [-1, 256, 2, 2]             512\n",
            "            ReLU-207            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-208           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-209           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-210           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-211            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-212            [-1, 256, 2, 2]             512\n",
            "            ReLU-213            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-214            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-215            [-1, 256, 2, 2]             512\n",
            "            ReLU-216            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-217           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-218           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-219           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-220            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-221            [-1, 256, 2, 2]             512\n",
            "            ReLU-222            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-223            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-224            [-1, 256, 2, 2]             512\n",
            "            ReLU-225            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-226           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-227           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-228           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-229            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-230            [-1, 256, 2, 2]             512\n",
            "            ReLU-231            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-232            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-233            [-1, 256, 2, 2]             512\n",
            "            ReLU-234            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-235           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-236           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-237           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-238            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-239            [-1, 256, 2, 2]             512\n",
            "            ReLU-240            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-241            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-242            [-1, 256, 2, 2]             512\n",
            "            ReLU-243            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-244           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-245           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-246           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-247            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-248            [-1, 256, 2, 2]             512\n",
            "            ReLU-249            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-250            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-251            [-1, 256, 2, 2]             512\n",
            "            ReLU-252            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-253           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-254           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-255           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-256            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-257            [-1, 256, 2, 2]             512\n",
            "            ReLU-258            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-259            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-260            [-1, 256, 2, 2]             512\n",
            "            ReLU-261            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-262           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-263           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-264           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-265            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-266            [-1, 256, 2, 2]             512\n",
            "            ReLU-267            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-268            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-269            [-1, 256, 2, 2]             512\n",
            "            ReLU-270            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-271           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-272           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-273           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-274            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-275            [-1, 256, 2, 2]             512\n",
            "            ReLU-276            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-277            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-278            [-1, 256, 2, 2]             512\n",
            "            ReLU-279            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-280           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-281           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-282           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-283            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-284            [-1, 256, 2, 2]             512\n",
            "            ReLU-285            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-286            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-287            [-1, 256, 2, 2]             512\n",
            "            ReLU-288            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-289           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-290           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-291           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-292            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-293            [-1, 256, 2, 2]             512\n",
            "            ReLU-294            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-295            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-296            [-1, 256, 2, 2]             512\n",
            "            ReLU-297            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-298           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-299           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-300           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-301            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-302            [-1, 256, 2, 2]             512\n",
            "            ReLU-303            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-304            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-305            [-1, 256, 2, 2]             512\n",
            "            ReLU-306            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-307           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-308           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-309           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-310            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-311            [-1, 256, 2, 2]             512\n",
            "            ReLU-312            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-313            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-314            [-1, 256, 2, 2]             512\n",
            "            ReLU-315            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-316           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-317           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-318           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-319            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-320            [-1, 256, 2, 2]             512\n",
            "            ReLU-321            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-322            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-323            [-1, 256, 2, 2]             512\n",
            "            ReLU-324            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-325           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-326           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-327           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-328            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-329            [-1, 256, 2, 2]             512\n",
            "            ReLU-330            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-331            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-332            [-1, 256, 2, 2]             512\n",
            "            ReLU-333            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-334           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-335           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-336           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-337            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-338            [-1, 256, 2, 2]             512\n",
            "            ReLU-339            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-340            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-341            [-1, 256, 2, 2]             512\n",
            "            ReLU-342            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-343           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-344           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-345           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-346            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-347            [-1, 256, 2, 2]             512\n",
            "            ReLU-348            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-349            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-350            [-1, 256, 2, 2]             512\n",
            "            ReLU-351            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-352           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-353           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-354           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-355            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-356            [-1, 256, 2, 2]             512\n",
            "            ReLU-357            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-358            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-359            [-1, 256, 2, 2]             512\n",
            "            ReLU-360            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-361           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-362           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-363           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-364            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-365            [-1, 256, 2, 2]             512\n",
            "            ReLU-366            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-367            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-368            [-1, 256, 2, 2]             512\n",
            "            ReLU-369            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-370           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-371           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-372           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-373            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-374            [-1, 256, 2, 2]             512\n",
            "            ReLU-375            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-376            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-377            [-1, 256, 2, 2]             512\n",
            "            ReLU-378            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-379           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-380           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-381           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-382            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-383            [-1, 256, 2, 2]             512\n",
            "            ReLU-384            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-385            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-386            [-1, 256, 2, 2]             512\n",
            "            ReLU-387            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-388           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-389           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-390           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-391            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-392            [-1, 256, 2, 2]             512\n",
            "            ReLU-393            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-394            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-395            [-1, 256, 2, 2]             512\n",
            "            ReLU-396            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-397           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-398           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-399           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-400            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-401            [-1, 256, 2, 2]             512\n",
            "            ReLU-402            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-403            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-404            [-1, 256, 2, 2]             512\n",
            "            ReLU-405            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-406           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-407           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-408           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-409            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-410            [-1, 256, 2, 2]             512\n",
            "            ReLU-411            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-412            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-413            [-1, 256, 2, 2]             512\n",
            "            ReLU-414            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-415           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-416           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-417           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-418            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-419            [-1, 256, 2, 2]             512\n",
            "            ReLU-420            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-421            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-422            [-1, 256, 2, 2]             512\n",
            "            ReLU-423            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-424           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-425           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-426           [-1, 1024, 2, 2]               0\n",
            "      Conv2dAuto-427            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-428            [-1, 256, 2, 2]             512\n",
            "            ReLU-429            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-430            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-431            [-1, 256, 2, 2]             512\n",
            "            ReLU-432            [-1, 256, 2, 2]               0\n",
            "      Conv2dAuto-433           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-434           [-1, 1024, 2, 2]           2,048\n",
            "ResNetBottleNeckBlock-435           [-1, 1024, 2, 2]               0\n",
            "     ResNetLayer-436           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-437           [-1, 2048, 1, 1]       2,097,152\n",
            "     BatchNorm2d-438           [-1, 2048, 1, 1]           4,096\n",
            "      Conv2dAuto-439            [-1, 512, 2, 2]         524,288\n",
            "     BatchNorm2d-440            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-441            [-1, 512, 2, 2]               0\n",
            "      Conv2dAuto-442            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-443            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-444            [-1, 512, 1, 1]               0\n",
            "      Conv2dAuto-445           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-446           [-1, 2048, 1, 1]           4,096\n",
            "ResNetBottleNeckBlock-447           [-1, 2048, 1, 1]               0\n",
            "      Conv2dAuto-448            [-1, 512, 1, 1]       1,048,576\n",
            "     BatchNorm2d-449            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-450            [-1, 512, 1, 1]               0\n",
            "      Conv2dAuto-451            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-452            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-453            [-1, 512, 1, 1]               0\n",
            "      Conv2dAuto-454           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-455           [-1, 2048, 1, 1]           4,096\n",
            "ResNetBottleNeckBlock-456           [-1, 2048, 1, 1]               0\n",
            "      Conv2dAuto-457            [-1, 512, 1, 1]       1,048,576\n",
            "     BatchNorm2d-458            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-459            [-1, 512, 1, 1]               0\n",
            "      Conv2dAuto-460            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-461            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-462            [-1, 512, 1, 1]               0\n",
            "      Conv2dAuto-463           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-464           [-1, 2048, 1, 1]           4,096\n",
            "ResNetBottleNeckBlock-465           [-1, 2048, 1, 1]               0\n",
            "     ResNetLayer-466           [-1, 2048, 1, 1]               0\n",
            "   ResNetEncoder-467           [-1, 2048, 1, 1]               0\n",
            "AdaptiveAvgPool2d-468           [-1, 2048, 1, 1]               0\n",
            "          Linear-469                   [-1, 10]          20,490\n",
            "   ResnetDecoder-470                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 58,164,298\n",
            "Trainable params: 58,164,298\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 10.60\n",
            "Params size (MB): 221.88\n",
            "Estimated Total Size (MB): 232.49\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = resnet152(3, 10)\n",
        "summary(model.cuda(), (3, 32, 32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoWe9aVHiY48"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Resnet152_Training_CIFAR10_normalize_batch32_epoch100.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ec1a59b962244b6ea398e3e576f6f31b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c7a1210cd454d728c61a626beb9e9a9",
              "IPY_MODEL_c2cccbe4fe8d4713a20d8bdb4ae19a4c",
              "IPY_MODEL_9ea1753c12c44df29cde6112332dc8ed"
            ],
            "layout": "IPY_MODEL_f1b5669946a94f058c71acbc697448a0"
          }
        },
        "2c7a1210cd454d728c61a626beb9e9a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef097fdb2e8a489289f347b025115840",
            "placeholder": "​",
            "style": "IPY_MODEL_56111eb22e2d41b5b48525b2c18b4aa7",
            "value": ""
          }
        },
        "c2cccbe4fe8d4713a20d8bdb4ae19a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8910f5787e904691b0ad378975376e0a",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2a8b53a87f64ac09d40ee92446de645",
            "value": 170498071
          }
        },
        "9ea1753c12c44df29cde6112332dc8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_347e39364e1c460cae53cf331422910d",
            "placeholder": "​",
            "style": "IPY_MODEL_61569ac89ad441979dde9d62629c53a7",
            "value": " 170499072/? [00:03&lt;00:00, 50472526.90it/s]"
          }
        },
        "f1b5669946a94f058c71acbc697448a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef097fdb2e8a489289f347b025115840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56111eb22e2d41b5b48525b2c18b4aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8910f5787e904691b0ad378975376e0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2a8b53a87f64ac09d40ee92446de645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "347e39364e1c460cae53cf331422910d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61569ac89ad441979dde9d62629c53a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}